Lec-8
EKS cluster ke 4 components hote hain..
1.Control plane
2.Worker nodes(EC2 instances that are managed by us)
3.Fargate Profiles (Instead of Ec2 we can run our app workload on fargate profiles)
4. VPC (although ye part nai hota hai EKS ka but still it plays a very crucial role eslye we counted vpc here)
$$$$$$$$$$$$$$$$$$$$$$
-> 
->Suppose hmare worker node jo hain vo pvt subnet me hain so in order to establis a connection between such worker nodes and control plane NAT Gateways setup
krna pdta hai. 
$$$$$$$$$$$$$$$$$$$$$$
Broader level pe jo EKS ka architecture hota hai:
EKS Control plane has atleast 2 api servers and 3 etcd which runs across three AZ within a region.
EKS control plane automatically detects unhealthy "Control plane" instances and replaces them.

Worker Nodes i.e EC2 instances
A node group is one or more ec2 instances deployed in an Ec2 ASG.
All instances in a node group should have same instance type,same AMI and use the same "EKS worker node IAM Role"

Fargate:
Fargate is serverless  ye sb to pata hai.
AWS Specially built Fargate controllers whose work is to identify  the pods belonging to fargate  and schedules them on fargate profiles

VPC
VPC is used for traffic flow from worker nodes to Control plane within the cluster,
EKS Control planes are highly isolated from other control planes present in the same AWS account.

<----------------------------------------->
<----------------------------------------->
<----------------------------------------->

Lec-9

Simple EKS cluster banane ka command run kiya hai, it takes around 15-20 min to create this.
Fir hme "IAM OIDC provider for our EKS Cluster" ko banakr associate krna hota hai, AWS Console me krne jaenge to bht
 sare steps hote hain eske (it is possible
that we make mistake there, eslye we will simply run a command on cli)

Esme command diya hai eks cluster creation ke,ye dekh kr he kabhi bhi banana cluster because simply eks create cluster kr dene
 se vo node group bhi
sath me bana deta hai and node group me jo ec2 instancse hain vo m4.large size ke hote hain
"--without-nodegroup" is imp

eksctl create cluster --name=eksdemo --region=us-east-1 --zones=us-east-1a,us-east-1b --without-nodegroup


In this way we are creating only cluster(basically control plane) separately, and after this we will be creating our node group separately
<----------------------------------------->

Lec-10
To enable and use AWS IAM roles for Kubernetes service accounts on our EKS cluster,
 we must create & associate OIDC(IAM Open ID Connect) identity provider.
eksctl utils associate-iam-oidc-provider \
    --region region-code \
    --cluster <cluter-name> \
    --approve

# Replace with region & cluster name
eksctl utils associate-iam-oidc-provider \
    --region us-east-1 \
    --cluster eksdemo1 \
    --approve
	
	
then eske bad hm Key-pair banaenge aws pe,bcz kbhi bhi incase hme kisi worker node se connect krna pde to we can do it via our key-pair
Worker node ultimately kya hain? hain to sb ec2 instance he na. SO unke liye key-pair banaya hai.

################# IMP
Now we will write the command to create worker node, See that command from github vala page:(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/blob/master/01-EKS-Create-Cluster-using-eksctl/01-02-Create-EKSCluster-and-NodeGroups/README.md)
Es command me ek jagh key-pair ka nam use hua hai , to vo vhi nam dalna hai jis name ki key abi hmne banayi thi aws console me

eksctl create nodegroup --cluster=eksdemo1 \
                       --region=us-east-1 \
                       --name=eksdemo1-ng-public1 \
                       --node-type=t3.medium \
                       --nodes=2 \
                       --nodes-min=2 \
                       --nodes-max=4 \
                       --node-volume-size=20 \
                       --ssh-access \
                       --ssh-public-key=kube-demo \
                       --managed \
                       --asg-access \
                       --external-dns-access \
                       --full-ecr-access \
                       --appmesh-access \
                       --alb-ingress-access 

####
Node group ban jane ke bad we can verify with commands like , kubectl get nodes
kubectl get nodes -o wide
	
	
<----------------------------------------->
Lec 11
Sbse pehle to ye check krnge ke hmara eks cluster ka jo nodegroup hai vo Public subnet me hai ya nahi.
Ye aws console pe check krna hota hai, lect dkh kr he smjh aega


eksctl ke thrrogh we should enable the logging of our eks cluster, it will help us in longer run


github pe 4-5 commands diye hain like to View: 1. List EKS Clusters 2.List NodeGroups in a cluster  
3.to see the config context of our eks cluster:  kubectl config view --minify


##############
IAM role lagane ke do tareeke hote hain, one is ke jo bhi policies attach krni ho vo directly hm apne IAM role me dede, and vo IAM role EKS Cluster se directly attach kr den(which is our case just above)

Dusra tareeka hai that we  create a k8s service and a IAM role and then attach them and then define the container ke esko kya kya access dena hai, ye hm aage ke lectures me  dkhge


Es lecture me zyadatar verification dikhaaya hai ke bhaiya dekh lo sahi se ke NAT Gateway bana ke nhi bana AWS Console pe, IAM role bana ke nhi bana
CloudFormation stack bana k nhi bna

Then last me apne security group me inbound traffic from anywhere bhi kar diya hai,bcz agar nodeport service bhi use krenge jb
to uske liye bhi access from anywhere outside the world is required, so that we can test our application going forward

<----------------------------------------->

->jb bhi hm eks create cluster  ya fir eksctl create nodegroup ye sb command run krte hain to api call jata hai CloudFormation ko
Cloudformation he backend pe hmara ye sb cluster/worker nodes create krta hai.

-> Worker nodes me agar hme ye dekhna hai ke kon kon se inbound ports allowed hain to we need to see that security group jiske nam me "remote"
likha hai.

-> worker nodes ko outside internet se koi bhi insan access kr paye so for that we need to allow traffic All traffic from Anywhere 0.0.0.0/0
-> Nodeport service basically dynamic ip generate krta hai for our worker nodes and we are able to access them using
Worker_node_public_ip:dynamic_port

<---------------------------------->

Lec12 in Sec2 me pricing smjhayi hai EKS ki. Its imp becz sbi log interview me cost ki trf dhyan dete hain.

->Just to Note: Worker nodes ko hm normal ec2 instance ki trh stop/start nahi kr skte, so we need to delete the worker nodes(node group) if we are
not using it.
<---------------------------------->

Lec13
->Cluster deletion ke liye ek bht bht bht imp point hai ke kbi kbi hme agar apne cluster delete krna hai , and suppose kuch changes hmne 
manually bhi kar rakhe hain(to our resources that are created by EKS) apne aws console me EKS cluster me jakr, 
to first we need to roll back our changes(Like SG me ports allow kiye the, to
first we need to delete those rules manually, uske bad he hme cluster delete vala command chalana hai: 
eksctl delete cluster <cluster-name>)
It is v imp to roleback our manual changes before deleting the cluster


Going forward hmlog IAM role me bhi kuch policies attach krenge , 
but jb cluster delete krne ki baari aegi to fr hme manually pehle vo policies
detach krni pdegi uske bad he cluster sahi se delete hoega




If you want to delete only nodegroup then uske liye bhi ek command hai: eksctl delete nodegroup --cluster=<clustername> --name=<NodeGroupName>
agar hm ye krna bhul jate hain to fir hme cloudformation me jakr manually bache hue resources ko ek ek krke delete krna hoga.


Kbhi bhi cluster delete krne ke bad ek bar cloud formation dekh zrur lena ke ,
jaise EKS Cluster NAT gateway banaata hai and Network Load balancer bhi
banata hai ye sb check kr lena ke delete hua ya nahi.
<---------------------------------->
<---------------------------------->
<---------------------------------->

Lec 19
->Suppose Docker desktop download krliya windows/macOS me. fir ye command run kiya:
docker run --name app1 -p 80:8080 -d stacksimplify/dockerintro-springboot-helloworld-rest-api:1.0.0-RELEASE


->here 80 means hmare desktop ka port 80 ko hmne container ke port number 8080 se map kr diya hai.
i.e First port is LOCAL PORT and the second 8080 is the CONTAINER PORT

->V Imp to Note: So jo hmare system me localhost hota hai uska port hota hai 80
So jb hm map kr denge apne 80 ko container ke 8080 se, fir apne system pe hmne agar ye run kiya http://localhost/hello
to fir hm apne container ko access kr paenge.

-> if we write docker ps -a -q  , to hme container id mil jaengi directly of the stopped container without showing any sort of faltu ki info about the container

Lec20 me bhi Localhost ka exampole diya hai. 
Image ki retagging bhi dikhai hai, ke suppose hme koi image banayi docker_hub_account/image_name:v1
bad me hme laga ke v1 acha nahi lag rha to we retagged it to v1-Release and then we pushed this newly retagged image to docker hub

Lec21
docker stats command se we can display live stream of container resource usage strategy
docker top container-name se we can display the running process of a container
<---------------------------------->
<---------------------------------->
<---------------------------------->
Lec22
->at 3 min, Controller Manager ke components bataye hain ye mujhe nahi pta the pehle se: 
->Control plane ka ek component aur bhi aur bhi hota hai apart from Controller Manager, etcd, kube-scheduler,api-server: Cloud Controller manager
->It is basically for cloud, on-premise k8s infra me ye component nahi hota hai.
->Eske i.e cloud controller manager ke 2-3 components hote hain, vo directly video me jakr dekh lo.

->99% cases me ek pod me ek he container hota hai, (it is always recommended to have so)
kbi kbi ek pod me ek se zyada container hote hain to we call such containers : side-cars
->These side cars are used to support main container ex-main contianer k liye data pull krke lane ke liye, 
main container ke logs kahi push krne ke liye etc etc

Lec27
->single pod creation with just a command dikhaya hai i.e without creating any manifest. at 2:50
		->kubectl run <container-name> --image <repo-name/image-name>:<tag>

->EKS me troubleshooting ke liye "describe" command is very imp, because usse we can get
 to know the events occured in the pod etc etc aur sath me
 bht si details mil jati hain pod ke bare me
 kubectl describe pod <podname>
 
Lec28
BDSNA (4-5 min ka hai yr dkh dalo fatak se)
->3 ways se we can expose our pods to outside: ClusterIP(pods will be accessible within cluster),NodePort,
LoadBalancer(Specifically used with Cloud platforms)
-> NodePort service jo hai usme IP kis hisab se kam krti hain ye btaya hai at  2 min ke as pas 

Lec29 me Handson dikhaya hai NodePort service ka
CLI se ek command me he agar kisi pod ko NODEPORT service ke through internet ko expose krna hai to uske liye ye command use kar skte hain:
		->kubectl expose pod <Pod-Name>  --type=NodePort --port=80 --name=<Service-Name>
		->kubectl expose pod my-first-pod  --type=NodePort --port=80 --name=my-first-service

above command me we only gave service port i.e 80,basically system assumes here ke target port(i.e pod ka port) bhi 80 hai.
Fir "kubectl get service" karke check bhi kar skte hain ke hmari service bani ya nahi bani
# Get Service Info
kubectl get service
kubectl get svc
# Get Public IP of Worker Nodes
kubectl get nodes -o wide
Access the Application using Public IP
http://<node1-public-ip>:<Node-Port>

Now we can access our pod from internet using <public-ip-of-workernode>:<Node-port>					(nodeport mtlb vo jo 30000-32767 ke beech ki value thi)
Service jo hoti hai vo sare worker nodes pe jati hai i.e jitne bhi worker nodes honge sbpe ek sath implement hoti hai(na ki kisi ek individual worker node pe)

->agar hme service port kuch aur rakhna hai aur target port(i.e container port) kuch aur to fir hm vo bhi kar skte hain but uske 
liye hme alag se target port mention
karna pdega apne command me
		->kubectl expose pod my-first-pod  --type=NodePort --port=81 --target-port=80 --name=my-first-service3

-> We can cross check services by kubectl get svc ya fir kubectl get service


Lec30
Realtime me pod ke logs ko kaise dekhte hain ye dikhaya hai. Infact 1:45 min pe ek link dikhaya hai where we can see different options
used with kubectl log command, which is very useful while troubleshooting any pod

Connect to any pod
		->kubectl exec -it <pod-name> -- /bin/bash
Bina container ke andar jaye bhi hm container me commands run kr skte hain:
 kubectl exec -it <pod-name> <jo-bhi-command-chalana-ho>

		->Kisi bhi pod ki yaml file agar dekhni ho: kubectl get pod <pod-name> -o yaml
Similarly kisi bhi service(ClusterIP,nodeport etc) ki agar yml file dekhni ho to we can execute:  kubectl get service <service-name> -o yaml

Lec31
Deletion dikhaya hai pod ka aur services ka.
->if you want to get all the objects present in a namespace: kubectl get all


Lec32
Suppose hmare k8s pe hmne koi app host kr rakha hai, aur achank se traffic badh jata hai to we can handle the traffic by:
1-Update ReplicaSet number,
2-We can use kubectl scale command to do that, 
3-We can enable the HorizontalPodAutoscaler to autoscale the pods 

Lec32
		-> to get the replicaset: kubectl get replicaset 
		Further ye dikhaya hai ke RepicaSet ko as a Service kaise expose krte hain.
Basically we create a service here.

ReplicaSet ki yml file bhi dikhayi hai(vohi same Bhupinder vali)

ReplicaSet ko kisi kubectl command se nhi banaya ja skta hai, it can be created via yml file only
kubectl get replicaset
kubctl describe rs <replica-set-name>
Further lectures me  Replica set ko badhana dikhaya hai (i.e Number of desired pods), and  internet ke through access krna dikhaya hai. 
"ReplicaSet" and "Service" ko Delete krna dikhaya hai, Github pe doc ko check krlo usme ekdm saf saf achese commands likhe hain. 


Verify the owner of the pod by: kubectl get pods <podname> -o yaml


Lec34
Expose ReplicaSet as a Service
kubectl expose rs <replica-set name> --type=NodePort --port=80 --target-port=8080 --name=<NodePort Service Name>

Deletion bhi dikhaya hai 

Lec35
Whenever we create a Deployment bedefault a ReplicaSet is rolled out.
Deployment maintains the version of our app  
Rollout and Rollback options are provided.
We can scale scale up scale down
We can pause and resume a deployment (i.e agar hmeapne app me suppose 10 changes krne hain to we dont want ke ek change kre
 fir sare pods ko recreate karen
fir dusra change kare fir recreate, eslye pausing a deployment means,
 deployment ko pause krke eksath changes krlo ya fir ek ek krke he krlo jaisa theek lage
then deployment ko resume krdo,usse vo sare changes hmare container me reflect hojaenge)
We can do Canary Deployment (i.e live production environment me kisi change ko introduce krna)via Deployement

Lec36
kubectl create deployment <Deplyment-Name> --image=<Container-Image>
Deployement create krne ke bad ek he command se hm apne ReplicaSet ko scale up and scale down kar skte hain:

		-> kubectl scale --replicas=10 deployment/my-first-deployment 
		->kubectl get deploy   (es command se we can see our deployment)
Then Deployment ko as a Service expose krna dikhhaya hai (ek he command sehojata hai. {Similarly abi upar jaise replicaset ko expose kiya tha})
kubectl expose deployment <Deployment-Name>  --type=NodePort --port=80 --target-port=80 --name=<Service-Name-To-Be-Created>
kubectl get svc

Sir ne Deplyoment padhate time total 4 docker images ka use kiya hai so that vo hme version change krna i.e Rollback etc dikha saken.

Lec37
Update Deployment using Set Image option
Application ki image ko update krna sikhya hai from version1 to version2 using set image, All these things are imp. 
chaho to directly github repo se he padh lo
		->#Get Container Name from current deployment ki yaml file
		->kubectl get deployment my-first-deployment -o yaml

		->#Update Deployment - (Upar vale command se hmne container name nikal lenge,fir usko neeche vale command me use kr lenge,dhyan rahe we talking about Container Name and not Pod Name)
		->kubectl set image deployment/<Deployment-Name> <Container-Name>=<Container-Image> --record=true
		
IMPORTANT- jab bhi hm docker image ka version change krte hain ,basically ek new update rollout krte hain to fir by default ek new
 ReplicaSet create hojata hai.
we can see it by "kubectl get rs"


Kbhi bhi agar database ke containers ko update krna ho to there we canuse recreate strategy for updating the deployment,bcz recreate krne ka mtlb hia
sare pods ko stop karke eksth recreate krega but99.99 percent cases me we use default strategy i.e rolling update vali(one after the other)

Suppose hm describe krke dekhna chahte hain deployement ko to bhi we can do it
like this: kubectl describe deployment <deployemnt name> 
Ek trh se activity log mil jata hai esse


Suppose hm rollout history dekhna chahte hain apne deploynent ki:
kubectl rollout history deployment/<Deployment-Name>


Lec38
Updating application using Edit Deployment Approach(Last lect me hmne set-image vala approach dekha tha)
		->kubectl edit deployment/<Deployment-Name> --record=true
Kbi bhi deployment version history ko store krna ho to we write  --record=true, esse versioning ki details store hojati hain.


Suppose hmne apne deployment ki yml me change krdiya and then save bhi kr diya in vim editor
Then hme agar rollout ka status dekhna ho to :
kubectl rollout status deployment/<your-deployement-name>

Similarly jb hm teesra update dalenge apne deployment me hmare pas ek ReplicaSet automatically ban jaega firse
Note: jb hm kubectl get rs karte hain to hme ReplicaSets ke nam show hojate hain, and jb hm "kubectl get pods" karte hain to hme pods ke nam
dhyan dene vali bat ye hai ke pods ke nam me replicaSets ke naam ka bhi ek hissa hota hai.	


<----------------------------------------------->
Lec 39
Maine ab ye approach start kr di hai -> main pehle github ka page khud padh leta hun fir uske bad 2x ki speed pe lecture dekh leta hun
Yahan pe ek ek command likhne ka fayda nahi hai qki github ke page pe bht bht bht 2
che se ,saf saf aur kafi informative way me likha hai.
to I suggest ke usi page ko directly refer krlo.
es lecture me Rollback krna dikhaya hai.

kubectl rollout history deployment/<Deployment-Name>

Suppose hm ye dekhna chahte hain ke es particular revision me kya yml file thi? basically kya tha es revision me then we can give a specific 
number of the revision to know more details of it, Like this:
kubectl rollout history deployment/<Deployement-name> --revision=<jo sa bhi number dalna ho vo dal do revision ka>
Aisa krne ke bad ANNOTATIONS me reflect hojaega ke actually me kya change kiya gya tha es revision me


# Undo Deployment(i.e just pichle deployment pe agar vaps rollback krna ho to)
kubectl rollout undo deployment/my-first-deployment


# Rollback Deployment to Specific Revision:(first check revision number via kubectl rollout history deployment/<Deployment-Name> command)
kubectl rollout undo deployment/my-first-deployment --to-revision=3

IMP
Suppose I want to restart all of my pods present in my deployment due to some mem issues or for whatever cause we can do
kubectl rollout restart deployment/<your deployment  name>


Lec 40
Pausing deployement and making all changes and then resuming the deployment.
Note: jab bhi hm kubectl rollout history deployment/<our-deployment-name> 
likhte hain to jo sbse last me record ata hai vahi apna current deployment hota hai.

Ek bar pause command chala dene ke bad: kubectl rollout history deployment/<Deployment-Name>
agar hm koi change krte bhi hain(ex-deployment version badal dena i.e deployment me koi new image dal dena/
 ya fir ek change ka example hai-cpu and memory
pe cap laga dena for our deployment) to hmare containers restart nahi hote hain

iF we dont pause our deployement then suppose hm 3 changes krne hain apne deployement me to har change ke bad jaise he hm
deployment ko save krenge to sbhi existing pod gets terminated and recreated

How to pause a deployment:
kubectl rollout pause deployement <deployement-name>


Now whatever command we want we can run ,like suppose we want to change the image of our deployement using set image command,upar maine likha hua hai ye command in some lect.
kubectl set image deployement/<deployment-name> <image url> --record=true 

Since abi to hmne deployment ko pause krke rakha hai, we can verify that abi hmara deployment number badha nahi hoga, 
using this command: kubectl rollout history deployment/<deployement-name>

We can also set resource limit using kubectl i.e hmare kisi container ke liye hme agar koi limit set krni ho to we can do it like this:
kubectl set resources deployement/<deployement-name> -c=<container-name> --limits=cpu=200m,memory=512Mi
in the above command 512mb ram and 200 cpu is way more than our current infra in the tutorial, etna nhi krna hai.because bill bht aega nhi to



How to resume a deployment:
kubectl rollout resume deployment/<deployement-name>


How to delete a deployment
kubectl delete deployment/<deployement-name>

Similarly we need to delete the service as well using: sevice kubectl delete svc <service name>

<------------------------------------------------------->
Lec41


What are kuberenetes service: (I learned from gpt: pod,deployment,nodeport,configmaps and secrets,ingress,persistent volumes and pvc,statefulsets)
However KDaida sir mentioned: clusterIP, NodePort,LoadBalancer,Ingress,externalName
ClusterIP:used for communication b/w applications inside k8s cluster (ex frontendneed to communicate withbacken)
Ingress: it is an advanced Load Balancer which provides context path based routing,SSL,SSL redirect and many more(ex aws alb)
exteranlName: to access externally hosted apps,or databases inside your k8s cluster we can use externalName service,ye service cli commands se nahi ban skti ,we need towrite yml file for this


Ye imp hai ,dekh lo direct lec, coz esme bataya hai ke jaise k8s me koi app hosted hai,use agar aws RDS se bat krni hogi to we need to
 setup "externalName" service
aur agar pods ko apas me within cluster bat krni hogi to we need to setup ClusterIP service
similarly outside users ke liye NodePort service/Ingress/LoadBalancer Service 

<----------------------------------->
Lec42
(https://github.com/harshitpandey8587/kubernetes-fundamentals/tree/master/05-Services-with-kubectl)
jab bhi kbhi clusterIP service hm banate hain  using kubectl command then --type=ClusterIP dene ke zrurt nhi hoti hai bcz it is by default assumed by k8s

In this lect ingress controller pod yad hai GPT pe dekha tha? 
to vhi Nginx controller pod ki configuration dikhai hai.
basically ese 'reverse proxy' kehte hain.


We can also create custom docker image related to our reverse proxy. this is shown in this lect.
Vse ye lec dekh he lo qki custom docker image with reverse proxy configuration is only required in industry.

Industry job ke liye ye lec imp hai bht, esme k8s ke namespaces ke bare me bhi mention kiya hai,
Basically nginx ki conf file me backend deployment ka pura naam:8080 dena hota hai.
to since we are in the same namespace eslye sirf deployment ka nam dene se kam chal jata hai but if we are in diff namespace to fir pura Cluster DNS hostname dena pdta hai.
Yahi ek lecture imp hai. in practical terms. thosa sa confused hun main that how backend and frontend containers got connected?


Explaination: frontend ka jo docker file hai usme nginx ki conf file ko copy kiya hai.
jo conf file hai usme http://my-backend-service:8080
ye mention kiya hai basically backend deployement ki jo service hai i.e ClusterIP usko mention kar diya hai so that FE and BE can communicate #
with each other.

fir backend ki deployment ko scale up kr ke dikha diya hai: kubectl scale --replicas=10 deployment/my-backend-rest-app

bht acha lect hai, dkh lo maja ajaega

<---------------------------------->
<---------------------------------->
<---------------------------------->

Lec44
do tarah se list ko define krna dikhaya hai Manifest me, ye mujhe pehle se nai ata tha, mtlb confusition that ab clear hogya ahi

Vaise halke me na lo es yml file banane ko , jo ki bataya gaya hai lec 45, ,46,47,48 etc me, qki thoda bht changes rehte he hain apas me
to chaho to dekh lo bas , koi notes ki zrurt nhihai bas yaad rkhne k liye dkh lo.


Lec 45
apiVersion v1 likhte hain hmlog manifest me, to vo kon sa version chalra hai currently ye kaha se dkhte hain ye dikhaya hai esme.



Lec 46

Ek chiz notice krna that jb bhi service kiyml file likhte hain to usme selector: dete time matchLabels: nahi likhte hain
where as deployment ki yml file likhte time matchLables likhte hain for giving label and its values

Aur jo ports: hota hai usme bhi koi containerPort karke kuch nhi likhte hai, directly port se related 
info dete hain jaise name port targetPort nodePort


Lec46/47/48 me Yaml file banana dikhaya hai, Simple pod ka, ReplicaSet ka, Services(NodePort) ka,Deployment ka
[IMPORTANT] mje ye nai pata tha ke Deployemnt.yml alag banti hai and Services.yml alag , I mean hme agar kisi RS/Deployement ko agar as a 
Service expose krna hai
to uske liye specifically hme alag se ek services nam ki yml file banani hoti hai.
ye lectures kafi similar hain Bhupendra Rajput se, but still informative hain.

Ek sath 2 yml files bhi hm run kr skte hain: kubecctl -f apply file.yml -f file2.yml

gpt PE Deployment strategies dkh skte hain: rolling update, recreate fashion etc

Lec50
Es lecture me Deployment and Services ki Yaml file bana ke dikhai hai, This lec is imp for writing manifest in K8s
jaise har chez ka ek  profarma hota hai ke services ki yml banaenge to usme kya kya chiz hoti hai ..

Vaise he Deployment ka pattern kch thooooda sa alag hota hai.(Ex- Deployment yml me replicas,selector,template
(template ke andar metadata aur spec:)  ye teen object hote hain)
(Vaise he Services ki yml file me type,selector,ports yeteen object hote hain spec: ke neeche)

2x ki speed pe dkh lo lect, ache se pattern smjh ajaega
Mtlb jaise kisi POD ki yml file me alag trah se port ko allow kiya jata hai, vahi kis Services ki Yaml me Ports 
ko kisi alag trah se define kiya jata hai yaml file me.
(Yad hai Services ko export krne me --port aur --target-port define krna pdta tha? nahi yad hai to refer line number 129 in this file)


When we are writing Backend Cluster service port(i.e apne backend ek pods ke liye jo bhi service(eg ClusterIP type ki)
 vali yml likhenge usme name of the  Service is very imp, 
qki hmare case me hmne front me es hisab se coding kari hai ke it will only route the traffic to a service jiska naam "my-backend-service" hoga)
Lec 42 me jo kam CLI commands se kiya tha vhi kam ab yml files se krne ja rhe hain. simple hai sbkch


Ek chiz notice krna that jb bhi service kiyml file likhte hain to usme selector: dete time matchLabels: nahi likhte hain
where as deployment ki yml file likhte time matchLables likhte hain for giving label and its values
Aur jo ports: hota hai usme bhi koi containerPort karke kuch nhi likhte hai, directly port se related info dete hain jaise name port targetPort nodePort
<--------------------->

Lec52
Suppose koi folder hai xyz nam ka ,uske andar 4 yml file padi hai(directly present ,recursively present nhi), so we can apply each yml file
using the folder name like this: kubectl apply -f xyz/
similarly delete ka bhi hai: kubectl delete -f xyz/
we can verify using kubectl get all

<--------------------->
<--------------------->
<--------------------->
<--------------------->
Lec 53
(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/04-EKS-Storage-with-EBS-ElasticBlockStore)
EKS storage 
1. In-Tree EBS Provisioner(Depricated)
2. EBS CSI Driver
3. EFS CSI Driver
4. FSx for Luster CSI (basically used for persistent volume mounts)

EBS can be mounted with Ec2 and container instances,they are exposed as storage volumes that persist independently
We can dynamically change the config of the ebs volume attached to any instance
Handson ke liye ek case liya hai, es case ko krne ke liye we need to learn about Storagr Class,PVC,Config Map , Env Variable,
Volumes,VolumeMount  , ClusterIP Service,Deployment,etc
Sbhi ke manifest banane honge

To test this app sir has created a simple sringboot microservice
so that database sahi se storage ka use kr pa rha hai ya nhi for its CRUD operations

Lec 54
IAM policy banayi hai EBS ke liye,basically worker nodes jo IAM role use kr rhe hongi usme  ye IAM policies attach kr denge, so that worker nodes
present in the EKS Cluster can access the EBS volumes from AWS

Policy ka document dkha ho to seedha lect se dekhlo(kul mila kr 10-12 permissions di hai related to ebs, CRUD operations etc)

now we want to know the IAM role associated with our worked node,for that we can use this command:
kubectl -n kube-system describe configmap aws-auth
From output check: "rolearn"
Alternative tareeka hai (to find associated IAM role)simply aws console me se worker node ke description me dekh lojakr

in order to attach the ebs csi driver,hme github ki repo ka link hai usko install krna pdta hai.
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master"

jo bhi latest version hoga ebs csi driver ka ye vhi version install kr dega

Ye command maine yaha se liya hai(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/04-EKS-Storage-with-EBS-ElasticBlockStore/04-01-Install-EBS-CSI-Driver)
actually me ye k8s ki officail git repo se he fetch krke la rha hai

After installing if we want to verify the installed csi driver or not, we can check using:
kubectl get pods -n kube-system
Esme ebs-csi-controller and ebs-csi-node karke pods ban gye honge

<---------------------------->
Lec 55 (imp hai, 5 manifest banana dikhaya hai.) Storage Class, Persistant Volume Claim, Config Maps,
Sari yml files yaha pardi hain: https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/04-EKS-Storage-with-EBS-ElasticBlockStore/04-02-SC-PVC-ConfigMap-MySQL/kube-manifests

Abtk maine kbhi nhi dekhi thi ye sb yaml files
ye lec BDSNA, sari yml files download krke rakh lo kahi pe local system me

to see storage classes in k8s: kubectl get scale
To see Persistent volume claims: kubectl get pvc       (in our case status will be "Pending" bcz we allocated 
volumeBindingMode: WaitForFirstConsumer while defining storage class yml file)
<-------------------------------------------------------------------------------------->
Lec 56
In this lec we will create mysql deployment manifest, and esko banane ke sath we will also create env variables,volumes,volume mounts
Basically deployment ki yml file hai, but selector ke neeche strategy define kari hai(vhi strategy jo ek interview me puchi gyi thi)
There are two strategies 1. roling update 2.recreate

Es lect me jo yml file banayi hai deployment ki, link below:
(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/blob/master/04-EKS-Storage-with-EBS-ElasticBlockStore/04-03-UserManagement-MicroService-with-MySQLDB/kube-manifests/04-mysql-deployment.yml)
vo kafi lambi hai mtlb volume mount wgreh dikhaya hai
(basically do volume banaye hain [1.Persistant volume 2.ConfigMap for DB creation]
 and fir unko pod me mount kr diya hai) to ,ye lect dkh lo qki aise written
text se smjh nhi aega. bcz kaha kaha ke name same honge(mtlb jaise pvc ki yml file me jo nam diya hai 
vhi nam deployment ki yml file me bhi mention krna hai) ye imp hai

Since mysql ka ek he pod hoga pure cluster me, basically we are not going to scale it up or down
in this case while defining the ClusterIP sevice yml file , we can give ClusterIP: Node ,this None means while are going to use pod IP directly#
https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/blob/master/04-EKS-Storage-with-EBS-ElasticBlockStore/04-03-UserManagement-MicroService-with-MySQLDB/kube-manifests/05-mysql-clusterip-service.yml
BDSNA


ConfigMap ki ek yml file banayi thi,usme data jo hota hai vo key-value pair ke form me hota hai
key means the name of the file that you want to create and value means the content which you want in that file
to jb hmne config-map ki yml file ko apne pod ke volume me mount kr diya , to fir vo config-map vali jo file hogi vo  ban kar execute hojaegi
mountPath: /docker-entrypoint-initdb.d
Ye command imp thi while writing deployment.yml
Maine gpt se pucha tha to ache se smjh me agya tha config map ka sara system, es link pe gpt ka answer given hai: https://chatgpt.com/c/9d9cf57b-89b3-452c-b49a-cce0d46c8d81
Ekbar dekh lo yr,ache se ajaega config map ka system.


Lec 57
# Connect to MYSQL Database
kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -pdbpassword11
es command ko explain kiya hai in lec
kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -pdbpassword11
https://chatgpt.com/c/9d9cf57b-89b3-452c-b49a-cce0d46c8d81    (Es link pe dkh lo gpt se maine explanation bhi liya hai es command ka)

Lec58
Jab sir lec record kr rhe the(tb EBS CSI ka version 0.5 tha) to PV ki resizing and snapshots dono he chize Alpha version of k8s me thi mtlb development horha tha enpe
but abi maine check kiya that currently 1.3 version of EBS CSI is being used
To sir ne ye dikhaya hai ke PV resizing and snapshots ki yml file unhone bana kr rakh di hain github pe
agar zrurt pde to check kr lena

Lec59 
esme do yml files bana ke dikhayi hain(ek deployment aur ek NodePort service) for client side interaction i.e 

Lec60
es lec me postman ki help se api call karke dikhai hai ,basically hmare k8s cluster me jo database hai in container
uspe crud ops perform krke dikhaya hai
Problem Observation:
If we deploy all manifests at a time, by the time mysql is ready our User Management Microservice pod will be restarting multiple times due to unavailability of Database.
To avoid such situations, we can apply initContainers concept to our User management Microservice Deployment manifest.
problem soved in lec 64

<----------------------------------------------------------->
<----------------------------------------------------------->
<----------------------------------------------------------->
<----------------------------------------------------------->
<----------------------------------------------------------->
Lec 62
Ab yaha se notes nahi banaunga direct github  ki readme.md file se padh lo, time bht waste hora hai
agar notes banaunga bhi to bas commands ke

secret.yml me db ka password dal diya hai base64 encode krke,
aur deployment.yml file me env variable me secret ki value dal di hai, so basically deployment.yml ke env variable mechange hua hai ,dkhlena

Lec 64
Init containers in k8s
these are side car containers jo ki main app container ke pehle run krte hain, in order to install any sort of dependencies
there can be more than one init container in a deployement, only thing is ek ek karke chalte(i.e ek ke bad ek) hain ye init containers
and sare init containers ko successfully run hona zruri hai in order to start our main app container

Basically jo microservice vala depoyment.yml thi, usme problem aarhi thi ke vo pehle he start ho ja rha tha aur jbki hmara sql server vala pod
tbtk start nahi ho pa rha tha eslye gadbad hori thi(mtlb ki client is ready but server is not ready)
eslye microservice vali deployment file me initContainers mention kr diya hai with a shell script(shell script ka kam hai sql server vale pod ko ping krna)
To jaise he sql server up and runnign ho jata hai , uske bad microservice vala pod (jo ki acting as a client) vo bhi create hojata hai



kubectl get pods -w
this command means we can get the pods in watch mode,mtlb we can see the execution


Lec 65
3 type ke probes hote hain k8s me

1. Liveness probe: kubelet uses liveness probe to knw when to restart a container.
LP could catch a deadlock where an app is running but unable to make progress,
the LP restarts containers in such cases ,basically restarting helps in such cases

2.Readiness probe:Kubelet uses readiness probes to know when a container is ready to accept traffic,
So kul milakr jbtk readiness probe successful nhi hojati we dont allow that pod to accept traffic.When a pod is not ready it is removed from
Service Load Balancers based on readiness probe signal..

3.Startup Probe:Kubelet uses readiness probes to know when a container applicaton has started.
Firstly this prob DISABLES livenss & readiness probes until it succeeds ensuring those pods dont interfere with app startup
This can be used to adopt liveness checks on slow starting containers, avoinding them getting killed by the kubelet before they are up and running
<-------------->
3 tarike se probes ko define kiya ja skta hai,lec dekh lo bhosdoooo.

1. check using commands
2. check using httpd GET request
3.check using TCP
but ye depend krta hai hmare app architecture pe ke hm kis hisab se prob ko implement krna chahte hain.

Lec66: me demo krke dikhaya hai liveness and readiness probes ka


Lec67:
K8s resources request and limits
basically resource limit for our containers inside the pod

		resources:
            requests:
              memory: "128Mi" # 128 MebiByte is equal to 135 Megabyte (MB)
              cpu: "500m" # `m` means milliCPU
            limits:
              memory: "500Mi"
              cpu: "1000m"  # 1000m is equal to 1 VCPU core  
			  
Resource limit bas smjhane ke liye btaaya hai because its obvious ke hmlog autoscaling (vertical /horizontal) enable karke he rakhte hain
eks clusters me, to agr autoscaling enabled hai to fr resource limit ka koi matlb nahi reh jata hai.
qki automatically resource autoscale hojate hain fr

We will remove this part of yml file in upcoming videos bcz limit laga dene se hmare t3.medium worker nodes pe laod pdega when we are trying to scale up our number of pods
to fr vo pending state me chale jaenge

Lec68
Namespaces:
 k8s and kdaida sir encourages alot to use namespaces
 
 generally 4 namespaces log banate hain industry me dev,qa,staging,prod
 
 Benefits: creates isolation boundary from other k8s object
			we can limit the rsources like cpu memory on per namespace basis
			
To get the list of existing namespaces:			
kubectl get namespace

kube-system: allthe k8s system related objects are created under this namespace
there are two more namespace that are already present in k8s they are kube-node-lease and kube-public

kubectl get all --namespace kube-system     (ya fr aise bhi likh skte hain kubectl get all -n kube-system)

To get existing namespaces:
kubectl get ns

How to create a namespace using cli command:
kubectl create namespace <name-of-namespace>

UseMgmg NodePort service ki yml file yad hai? usme hmne nodePort diya tha ek jagah 312321
what is this 31231? ye hmare nodePort tha 
but jb hm ek se zyada namespace use krenge cluster me to fr to problem hojaegi na
bcz one port cannot be shared for more than one namespaces
afetr commenting this nodeport in service.yml is going to be dynamic 

whenever you want to apply any manifest in any particular namespace then,write this command
kubectl apply -f <folder-name> -n <namespace-name>


IMP: in order to access our application presend in some namespace hme node ki ip chaiye hogi,
we can get nodeip via: kubectl get nodes -o wide

Service port janne ke liye we will execute below command:
kubectl get svc -n <namespace-name>

Catch: jo PV aur SC hote hain these are not bound to be in a namespace mtlb ye generic k8s objects hain to we can simply see them using kubectl get pv,sc

SC and PV are not specific to a namespace,however jab enhe query krte hain hm to usme path likh kr ajata hai ke kis namesapce me attached hain ye

Delete everything that is present in a namespace:
kubectl delete -f <folder-name> -n <namespace-name>
There is another way to do it as well
like we can delete the namespace itself: kubectl delete ns <namesapce-name>

Check kr lena shyd storage class ko alag se delete krna pdta hai


Lec70
Last time abi resource  limit lagayi thi ek container me in terms of cpu and memory
Bar bar vo code snipped har container ki yml me na likhna pde eslye we can simply crate a LimitRange.yml file,but make sure to place it in dir in such a way that sbse pehle vhi execute hoega

aur we can associate this yml file to our namespace, usse ye hoga ke ek particular namespace me agar koi bhi  container banega to fir vo usi
resource limit ke sath banega.
Yml file dikhai hai es lect me limitRange ki

Lec71 me likhkr dikhaya hai yml
K8s terraform ki trh intelligent nhi hai to alphabetical order me manifest create krta hai .


Lec73#
Resource Quota
Namespace ke upar hard limits bhi laga skte hain hmlog using Resource Quota
Like es namespace me 5 se zyada pod nahi banne chaiye, memory limit ye hogi, pvc bhi 5 ho skte hain max, secrets bhi 6 he ho skte hain es
namespace me bas, es trh ki limits laga skte hain on any namespace

yml file dikhayi hai resource quota ki,suppose hmne yml file bana li hai then we can search for Resource quota like this:
kubectl get quota -n <namespace-name>

<------------------------------------------------->
<------------------------------------------------->
<------------------------------------------------->
<------------------------------------------------->

Lec 74
Abtk jo hmne sql server vala container bana kr crud ops kr rhe the it is extremly bad practice
(jis az me ebs tha usi az me sql server vala pod bhi hona chiaye etc etc aur b bht demerits hain), we should opt for aws RDS
externalName service yad hai? cluster ke andar ke resources ko agar bahr ki koi aws service use krni hotihai to we deploye externalName service

[IMPORTANT]
Es lec me new architecture dikhaya hai that how everying is places within VPC / EKS Cluster / public Subnets/ RDS
Database should always be present in private subnet
abi to hmlog ek front facing container ko public subnet me rakh de rhe hain but aage chalkr,hm aage ke lectures me dekhenge that
sare containers pvt subnet me he bnenge and public subnet me ALB hoga bas.

aisa he kch bola h sir ne lec me

Lec75
pvt subnet me rds instance banane k liye ek SG lagega,to vo SG bana kr dikhaya hai aur ports allow kiye hain 3306 on aws console
RDS me jakr DB-Subnet-group create kiya hai on aws console[ye mujhe pehle se nhi  pta tha, i.e RDS instance banane k liye hme Subnet-group bbi create krna pdta hai,
basically hmlog apne pvt subnets ko select krte hain usme bcz we want rds instance to be created in pvt subnet]

While creating RDS instance hmlog apne SG ko mention krenge jo abhi upr esi lecture me banaya tha and subnet group ko bhi mention krenge

We will also see that how to connect to our database instance (from within the cluster)
bahar se nhi access kr skte hain bcz we defined our rds instance in pvt subnet

RDS instance ban jane ke bad uska endpoint copy krlena ,bcz we will use this endpoint while configuring external name service
yml file rakh di hai es folder me local system me(ghar ke laptop me)(D:\DevOps study\AWS\EKS Microservice\EKS-MicroService\yml files\06-EKS-Storage-with-RDS-Database)

kubectl get svc

jab ye externalName service bana di hai ,uske bad ek new container bana kr test  kiya hai that whether we are able to access our RDS instance or now
kubectl run -it --rm --image=mysql:5.7.22 --restart=Never mysql-client -- mysql -h <endpoint-of-rds-instance> -u dbadmin -pdbpassword11
dbadmin is username of db and passwrd jo hai vo rds instance banate time diya tha.
mysql-client is the container name

<------------------------------------------------->
<------------------------------------------------->
<------------------------------------------------->
<------------------------------------------------->
Lec77
[IMPORTANT] Architecture dikhaya hai, dkhlo idea lag jaega, bas ALB ki jagh classic LB use kr liya hai.
lkin fr bhi architecture dkhlo ek nzr

abhi tk hmare worker nodes jo the vo public subnet me the, but now we want worker nodes to bein pvt subnet for that we are deleting exisitng worker nodes
and then recreating them in pvt subnet

eksctl get nodegroup --cluster=eksdemo1
eksctl delete nodegroup <Nodegroup-name> --cluster <cluster-name>

--node-private-networking
bas ye tag add kr dena while writing that (eksctl create nodegroup --cluster=eksdemo1 \ vala )command

Lec79
Jo service.yml hoti hai usme service me NodePort likha hua tha usko hata kr Loadbalancer likh diya hai
and nodePort:31231 jo likha tha ye line hata di hai
in order to attach a loadbalancer

Sir aisa bhi bolre the ke nodePort sevice ko expose krna in prod env is not good practice bcz DNS wgreh hota hai actual prod me.
aur jo port tha 8095 usko change krke 80 kr diya hai.

spec:
	type: LoadBalancer
	selector:
		app: usermgmt-restapp
	ports:
      - port: 80   	80 is the default port of the browser, eska mtlb ab hme jb b application ko reach krna hoga we can simply use LB ka Link
	    targetPort: 8095

Lec80
Creating NLB via manifest
same rhengi sb manifest as used in CLB, bas ek he manifest me change hoga 
I have placed that in local system:

  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb    # To create Network Load Balancer

<------------------------------------------------->
<------------------------------------------------->
<------------------------------------------------->
<------------------------------------------------->
Lec82
tne sare topics cover krna ja rhe hain sir, maja ajaega padh kr:
 AWS LB Controller install,ingress,context path routing,
ingress SSL, ingress SSL Redirect, External DNS install,Ingress+ External DNS,k8s Service + External DNS,Ingress Name based Virtual Host Routing
SSL Discovery -TLS,Ingress Groups,Intress Target Type -IP,Ingress Internal ALB

Sbke bare me one-liner intro bhi diya hai sir ne.chaho to dekh lo,4 min kalec hai
<------------------------------------------------->
Lec83
ALB ke benefits bataye hain like it supports path-based routing,host-based routing,support for containerized applications(ECS),monitoring health of each service
support for registering targets by IP address,including targets outside the VPC for the load balancer(VERY IMP)

(https://eksctl.io/getting-started/)
eksctl ki official website ko refer kr skte hain for future, kbhi b zrurt pde to

ALBIC(ALB Ingress Controller)(Ab depricate ho chuka hai)
it triggers the creation of AWLB and necessary supporting aws resources
annotation used: kuberenetes.io/ingress.class: alb

ALBIC jo hai vo do type ki routing provide krta tha
1.INstance Mode: Traffic coming to ALB is routed to the underlying worker node ec2 instance
2.IP mode: Traffic coming to ALB is routed to the pod ip directly.(This is beneficial specifically in case of fargate profiles jaha pe ec2 instances nhi hote h)
Note: Fargate profiel ke sath jb EKS kam krta hai to Fargate does not support nodePOrt service
Whereas EKS with Ec2 me dono he modes lagaye ja skte hain Instance mode and IP mode
By default insatnce mode laga hota hai

[ARCHITECTURE hai ALBIC ka but dekhne ki zrurt nhi hai qki ye depricate hochuka hai]
Jo bhi ye ALB wghreh bante hain vo sb ingress manifest ki vjh se bante hain,so if we want to delete everything simply ingress manifest ko delete krdo


Lec84
ALBIC depcripate hogya hai ,now its renamed as AWS LoadBalancer Controller
pehle ALBIC jab banate the to bas ALB create hota tha
but with AWS LoadBalancer Controller, now we can also create Network LoadBalancer for our cluster and route traffic accordingly.


For creating ALB we are going to use K8s object named as : Ingress Resource
For creating NLB we are going to use K8s object of type SERVICE and make sure to annotate it with "nlb"

[ARCHITECTURE Dikhaya hai -dekh lo imph h]

AWS LB Controller jb hm banate hain to uske liye k8s ke andar ek service account banana pdta hai,service account ke andar AWS IAM Role ka nam
annotate krna pdta hai, (yes ofcouse IAM ROLE me policies bhi attach krni hoti hain so that hmara "AWS LB Controller" ek new ALB ko create/update/delete kr paye on our aws account)
Once we create AWS LB Controller,backend pe AWS LB Controller Deployment banta hai, AWS LBC WebHook Cluster IP Service banti hai,AWS LB TLS secret banti hain

Lec85
kubectl ke sever version aur client version se related ek bat batayi hai, dkhlo chudau.

Fir eksctl se related commands ko revisitkiya hai yahan(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/08-NEW-ELB-Application-LoadBalancers/08-01-Load-Balancer-Controller-Install)
https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/08-NEW-ELB-Application-LoadBalancers/08-01-Load-Balancer-Controller-Install
eksctl get nodegroup --cluster=eksdemo1
eksctl get iamserviceaccount --cluster=eksdemo1

Configure Kubeconfig for kubectl:
aws eks --region <region-code> update-kubeconfig --name <cluster_name>
Like This: aws eks --region us-east-1 update-kubeconfig --name eksdemo1

The place where kubeconfig file resides is: /home/<user-name>/.kube/config

Troubleshoot,kbhi agar hm apne local system se apne remote eks cluster pe koi command hit krte hain like
 we execute kubectl get pods,aur output nahi aa rha hai to fir update your kube config file with the above command.
 Esse hota ye hai ke hmara local system sync me ajata hai remote eks cluster ke
 
 
Lec86
ek iam policy document hai github peusko download krke IAM policy bana li hai using AWS CLI command,
(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/08-NEW-ELB-Application-LoadBalancers/08-01-Load-Balancer-Controller-Install)
now ab ek single eks command se hmlog ek k8s service account banaenge and ek IAM role banaenge and we will annotate the 
service account with the IAM Role
all this will be performed with single eks command. 

We will check ke pehle se koi k8s service account(with this name 'aws-load-balancer-controller') to nhi present hai kube-system namespace me.
kubectl get sa aws-load-balancer-controller -n kube-system

we can check what all service account(sa) are present in kube-system namespace using below command:
kubectl get sa -n kube-system

Now vo command jo sa aur IAM Role create krega aur iam role me policy ko bhi attach kr dega, and yes sa aur IAM role ko bind krega eksath
eksctl create iamserviceaccount \
  --cluster=my_cluster \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \ #Note:  K8S Service Account Name that need to be bound to newly created IAM Role
  --attach-policy-arn=arn:aws:iam::111122223333:policy/AWSLoadBalancerControllerIAMPolicy \
  --override-existing-serviceaccounts \
  --approve
  
Now verify the sa bana ya nhi
eksctl  get iamserviceaccount --cluster eksdemo1

Now after creation we can also describe the newly created service account like this:
kubectl describe sa aws-load-balancer-controller -n kube-system


Lec87 
Yaha se notes banana tough hogya hai ,eslye directly dkhlo
Helm wgreh dikhaya hai.
https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/08-NEW-ELB-Application-LoadBalancers/08-01-Load-Balancer-Controller-Install

https://chatgpt.com/c/a78c463e-f909-4c78-8cbf-c426f73e3126  -> es chat me maine helm gpt se helm ka overview liya hai


# Add the eks-charts repository.
helm repo add eks https://aws.github.io/eks-charts

# Update your local repo to make sure that you have the most recent charts.
helm repo update

Generic syntax of installing a release using helm chart
helm install <release-name> <repo-name>/<chart-name>


Helm command to deploye aws-load-balancer-controller
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=<cluster-name> \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=<region-code> \
  --set vpcId=<vpc-xxxxxxxx> \
  --set image.repository=<account>.dkr.ecr.<region-code>.amazonaws.com/amazon/aws-load-balancer-controller


Ye above vala command run krne ke bad we can check for the available deploymentin our kube-system namespace, ek new depoyment ban gya
hoga with the name aws-load-balancer-controller


Lec 88
Ye lec dekh dalo qki aws-load-balancer-controller banane ke bad jo bhi chize backend pe deploy hoti hain(webhook clusterIP service,deployment,secret-tls)
en sbko verify krke dikhaya hai using commands and -o yaml files.

Lec89
Esme bhi verification he dikhaya of the underlying resources created due to AWS LB CONTROLLER creation

Lec 90
Uninstall aws lb controller using HELM command
This is only for info purpose,we dont need to execute this command
helm uninstall aws-load-balancer-controller -n kube-system 

Lec91
Ingress Class
If we have multiple ingress controllers running in our k8s cluster then how to identify to which ingress controller our ingress resource service should be associated to?#
IngressClass ki yml file banayi hai,I have downloaded it in local system.
(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/08-NEW-ELB-Application-LoadBalancers/08-01-Load-Balancer-Controller-Install)
There are many ingress controllers present in the market.
IngressClass ki yml file me ek controller:  hota hai usme hm mention krte hain ke kon sa ingress controller se hm associate krna chahte hain apne ingress resources ko

Video me ingress resource ki bhi yml file dikhayi hai.but aage lectures me ache se explain krenge 
Ye lec dkhlo to ache se smjh ajaega yml file of ingress
However gpt se maine pucha tha ingress class and resource ke bare me (https://chatgpt.com/c/5212e831-1de0-4e75-8b38-76851986d052)

# Verify IngressClass Resource
kubectl get ingressclass

# Describe IngressClass Resource
kubectl describe ingressclass my-aws-ingress-class

Lec93
[IMPORTANT ARCHITECTURE] Yahi to dhund rha tha main etne din se
Ingress jo hai uske 2 backends ho skte hain ek defaultBackend and ek me hm khud ke rules dete hain
but dhyan se pattern notice kroge to 
service:
	name: <nodePort Service Name>
	port:
      number: 
	  
	  Etna pattern same he hai
	  
Lec94
Ingress controller aur ingress resoruce ka demo krna hai hmlog ko basically to uske liye ek deployement aur nodePort service ki bhi zrurt pdegi
to es lect me bas deployment aur nodePort service bana kr dikhayi hai.

Lec95
Creation of ingress resorucemanifest with 'defaultBackend'
bht acha lec hai, esme dikhaya hai ke annotations: section me jobhi details hm dete hain vo sb aws pe infra bankar ajata hai 
(mtlb jse agar ALB bana rhe hain to ALB se related bht se parameters diye hain in annotations block)
puri ingress resource ki yml bana kr ache se dikhaya hai.

bas backend me specific path na lekr defaultBackend ka use kiya hai.

Lec96
Verification kiya hai after implementing ingressclass vali yml, aur ingress resource vali yml file
like AWS console pe ALB bana ya nhi bana ye sb dikhaya hai, target group me kya kya hai.
ports kon kon se open hai etc etc

Target-type dikhaya hai: i.e instance
2 type ke hote hain yaad hna? instance aur IP

na yaad ho to lec 93 dkh lena usme architecture banate smay bataya hai 

Lec97
badhiya lec hai regarding ingress with rules.
pehle mujhe bhi dikkat hui smjhne me ke bhai jab ALB ka url run kr rhe hain browser pe to nginx ka default page q aarha hai
bad me last me smjh agya
vo eslye aa rha tha qki rules me / mention kiya tha i.e koi bhi admi / esme jaega(i.e ALB ka url bas simply run krega to fir usko meri  nodePort service ki trf route kr do)

bad me sir ne jab / ki jagh /app1 kar diya in ingress resource manifest tb fr ALB ka url chalna band hogya.

<---------------------------------------------------->
<---------------------------------------------------->
<---------------------------------------------------->
<---------------------------------------------------->
<---------------------------------------------------->

Lec98
[IMPORTANT ARCHITECTURE]
Proper dikhaya hai kaise host based routing me ingress controller kaha pe hota hai, vo k8s ki apiserver ko monitor krta rehta hai for any incoming ingress service manifest
jaise he ingress resource ki manifest ati hai vo usi ke according ALB banata hai aws console pe
Note: ALB aur ingress service ek he chiz hai,mtlb ek he entity hai, bas aws ke context me smjhne ke liye hmlog use ALB keh dete hain.
aur k8s ke andar usko ingress service kehte hain.

INgress service jo hoti hai usi me rules likhe hote hain routing ke.

Lec99
Pehle yad hai ingress vali manifess me he hmlog annotations: vale block me healthcheck se related annotations dal dete the.
tb aisa eslye krte the qki our healthcheck was COMMON for all the backend applicatoins

but ab hmare pas backend me 3 app-server run ho rhe hain i.e 3 pod run ho rhe hain alag-alag services ke. PodA for app1 service,PodB for app2 service and Pod3
but suppose ab ek se zyda app run ho rhe hain hmare backen pe to fir ye jo healthcheck vala part hai ye Service level manifest me shift krna pdega
(qki hm har app-server ke lie alag se health check rkna chahte hain)
like jo NodePort service vali manifest hai har microservice ki,to usi me mention krna pdega health check


name:
labels:
annotations:

	alb.ingress.kubernetes.io/healthcheck-path: /app1/index.html

service manifest me ye line add hojaegi 

Es Lec me teeno app-server pods ke service.yml aur deployment.yml file ko dikhaya hai.
<--------------------------->
Lec100
Ingress service ki yml file bana kr dikhai hai,muje to ache se smjh aarha hai sbkuch ingress me.
context-path-routing dikhayi hai.
ke kaise ingress service ki yml file me rules likhe hain for diff apps deployed in our cluster

spec:
  ingressClassName: my-aws-ingress-class   # Ingress Class                  
  rules:
    - http:
        paths:      
          - path: /app1
            pathType: Prefix
            backend:
              service:
                name: app1-nginx-nodeport-service
                port: 
                  number: 80
				  

# Deploy Kubernetes manifests
kubectl apply -f kube-manifests/

# List Pods
kubectl get pods

# List Services
kubectl get svc

# List Ingress Load Balancers
kubectl get ingress
esko krne se hme ingressClass ka nam milta hai aur jo hmara ALB bana hota hai uska DNS-name bhi mil jata hai

# Describe Ingress and view Rules
kubectl describe ingress ingress-cpr-demo

# Verify AWS Load Balancer Controller logs
kubectl -n kube-system  get pods 
kubectl -n kube-system logs -f aws-load-balancer-controller-794b7844dd-8hk7n 

Further verification dikhaya hai es lect me jaise ALB jo bana hai aws console pe usme sahi se rules define hue hain ye nahi yai sb

Lec101
ingress rules define krne me priority order bht imp hota hai.
if we give /* as the first rule to problem hojaegi for remaining other rules.
eslye alwasy try to give the root url in the last rule among all the rules defined in ingress service

Ingress resources ka cleanup i.e deletion is v imp bcz eski vjhse he backend me ALB banta hai, and ALB costs us alot

IMP: Instead of defining a rule for ROOT CONTEXT, we can also give the defaultBackend (jo abi 2-3 lectures pehle padha tha)
Smjhe? nhi smjhe? mtlb ki ek he ingress service vali yml file me defaultBackend bhi mention kr dena aur sath me 'rules:' bhi mention kr dena

<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->

Lec102
[IMPORTANT ARCHITECTURE]
pura eks ka architecture dikhaya hai along with SSL(issued by ACM) and DNS i.e ROute53

Lec103
Route53 pe ek new domain name purchase krke dikhaya hai bas

Lec104
Imp hai agar tmhe dns aur ssl ko bind kaise krte hain apas me esme confusion hai
ek wildcard ssl certificate generate kiya hai ACM se via */<domain-name>.com
fir validation pending that to usko validate kiya hai, like simply CNAME records ko add krna pdta hai route53 me jakr hostedzone me jakr.


Lec105
Ingress service ki yml file me jo annotations hote hain usme ssl ko kaise mention krna hai ye sb dikhaya hai
Lec 100 me total 4  files thi , to 3 file (to as it is he rhengi bas jo ingress-service vali yml file thi usme kch changes kiye hain) 
jaise ingress-service ka nam change kiya hai, alb jo bnega aws console pe uska nam change kiya hai,
annotations me jakr 2 new chiz add kari hai  
ek to listen-ports(for our alb), dusra certificate-arn like this:

## SSL Settings
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}, {"HTTP":80}]'
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:180789647333:certificate/632a3ff6-3f6d-464c-9121-b9d97481a76b
    #alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-1-2017-01 #Optional (Picks default if not used)  

Fir ek subdomain banaya hai route53 me jiski destination me ALB(hmlg ka ingress ne jo alb banaya hai) usko mention kiya hai
mtlb ab jab bhi hm es subdomain ke sath koi path denge to fir hm destination pod pe phoch jaenge 

so that ab hmlog kisi sub-domain ke sath bhi securely apne pods ko access kr sakenge
<sub-domain>.<domain-name>.com/<context-path>

inour case context path was app1/index.html	 ,app2/index.html	

Lec106
[IMPORTANT ARCHITECTURE] esme SSL redirect kaha pe ata hai architecture me vo dikhaya hai
abi tk jb hm http:// krke apne domain name pe jare the to secured connection nhi tha, in this lec we will see how to redirect http to https

bas ye annotation add kr diya hai in ingress service annotation me   
# SSL Redirect Setting
    alb.ingress.kubernetes.io/ssl-redirect: '443'  

OFFICIAL WEBSITE FOR ANNOTATIONS: https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/ingress/annotations/

<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->

Lec107
[IMPORTANT ARCHITECTURE]  ExternalDNS
Eske pehle ke lectures me hmne khud manually jakr ROute53 me jo hosted zone tha usme DNS record add kiya tha i.e ek subdomain banaya tha 'A' Record ka and uska
target kiya tha ALB ko.
But with ExternalDNS these things can be automatically achieved just by adding a annotations(related to externalDNS) in k8s ingress service Or  in a k8s service, 
Ingress service ya suppose koi normal k8s service me annotation(related to externalDNS) add kr dene se automatically hmare route53 me DNS record add hojaega

Not only addition of records but we can also perform CRUD operations on dns records present in route53

Architecture smjhaaya hai,default namespace me ek service-account banana hoga for externalDNS, aws console pe iam policy banani hogi for crud ops on route53
then 
1.	externalDNS pod,
2.  externalDNS-deployment,
3.	externalDNS-clusterRole,
4.	externalDNS-ClusterRoleBinding will be deployed
(en charon chizo ki manifest hai sir ke pas, uski help se he ye deploy hongi)

Fir jaise he koi ingress service hm deploy krte hain to external-DNS-pod goes to k8s-apiserver and accordingly searches for the annotations(related to ExternalDNS)
and then CREATE/UPDATE/DELETE records  in route53

Lec108
Socho ingress service ki yml file kitni imp hai, bcz esi me hmlog annotation ke form me DNS records bhi likhte hain ,sath me neeche spec: me jakr rules: me context based routes bhi likhte hain
<subdomain>.<domain-name>.com/<context-url>

eksctl create iamserviceaccount \
    --name service_account_name \
    --namespace service_account_namespace \
    --cluster cluster_name \
    --attach-policy-arn IAM_policy_ARN \
    --approve \
    --override-existing-serviceaccounts

service account ban jane ke bad we can verify: kubectl get sa 
kubectl describe sa <service-account-name>

IAM Role jo banega uska arn chaiye hoga(bcz aage chalkr service account ki jo  manifest me ye mention krna hoega so that IAM role ki madad se Route53 me records add updatee hoskte),
 ab chaho to aws consoole se lelo ya fir ye command run krke bhi mil jaega
eksctl get iamserviceaccount --cluster <cluster-name>

Lec109
sabhi yml files dikhayi hain screen pe aur ye bhi dikhaya hai ke kaise configure krna hai external-dns ko
mujhe thoda kam kam smjh aaya hai abi, but theek hai jb kbi externalDNS ki zrurt hogi tb ki tb dkh lenge ye lec to ajaega step by step
yaha pe yml file hai charon chizon ki(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/blob/master/08-NEW-ELB-Application-LoadBalancers/08-06-Deploy-ExternalDNS-on-EKS/kube-manifests/01-Deploy-ExternalDNS.yml)


lec 111
ingress-service ke annotation vale block me es trh se hmlog de skte hain sub-domain ki entries:
  # External DNS - For creating a Record Set in Route53
    external-dns.alpha.kubernetes.io/hostname: dnstest1.kubeoncloud.com, dnstest2.kubeoncloud.com  

Ingress service ko deploy karke bht tareekon se verification dikhaya hai 
ke sahi se record add hua ya nhi hua, kya sahi se hmlog access kr pa rhe hain ya nhi kr pa rhe hain.

lec 112(DEPRICATED-mat padho esko)
k8s me mujhe ni pta tha ke 'LoadBalancer' naam ki bhi service hoti(basically ye Classic loadbalancer deploy krti hai aws me) hai(abtk mje nodePOrt,clusterIP, yahi pata thi)
in this lec ek service ki manifest hai(of type LoadBalancer) usme annotation me external-dns-name diya hai so that jb ye manifest chale to route53 me record add hojaega
aur LoadBalancer service deploy hojae


<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->
lec 113
[IMPORTANT ARCHITECTURE] NAME based Virtual Host Routing
Agar external-dns hmne configure kar rakha hai i.e LEC-109-110 ko implement kr rakha hai
to fir jaise jo ingress service vala manifest hai usme hm context based rules dete the? under rules:

to fir esme slight change hai yahi pe ,basically pehle rules: me hmlog directly http likhte the ab ,http ke just pehle ek - host: nam ka block aagya hai
jisme hmlog DNS record ko specify krenge i.e new sub-domain hm denge

 rules:
    - host: app101.stacksimplify.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: app1-nginx-nodeport-service
                port: 
                  number: 80
				  
				  
 then we can directly give the sub-domain name  which we want 
aur annotation block me simply bas default-page ka link de skte hain .(LIKE THIS:  external-dns.alpha.kubernetes.io/hostname: default101.stacksimplify.com )
githublink: https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/blob/master/08-NEW-ELB-Application-LoadBalancers/08-09-NameBasedVirtualHost-Routing/kube-manifests/04-ALB-Ingress-HostHeader-Routing.yml

Lec 114 
ingress service manifest ko deploy karke dikhaya hai sath me trh trh ke verification dikhaye hain, k bhaiya dkh lo sb ache se deploy hogya hai.

<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->
Lec115
so ye maan ke chalte hain ke external-dns to hmne configure krke rakha hai(lec 108-109) (all that service account blah blah blah)
to hmlog ingress service manifest me annotation me apne ACM ke certificate ka ARN mention karte the.
so that hme SSL vala tala dikhayi parde.

but agar hmlog Host-header-based-Routing(i.e lec 113-114 me jo seekha hai) ka use kr rhe hain to fir hmlog vo annotation hata skte hain ingress service manifest se
bcz host: me jb hm domain name de he rhe hain to fr ssl certificate vo automatically fetch kr leta hai hmare sub-domain ke liye.

what we only need to do is we have to add this in 
spec:
	tls:
	  - hosts:
		- "*.stacksimplify.com"
		
		ye krne ke bad we can comment ACM vala annotation from ingress service within annotaion block
<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->
Lec118
abtk hmlog backend pe 3 apps ko manage kr rhe the,but ingress service manifest ek he thi
usi me sare rules,annotaions, etc etc sb likh rhe the ek akeli bechari ingress service manifest me
but suppose production me 50 apps hon backend me sbki apni apni rules ,sub-domains, health-checks n ol hon to?
tb to bht confusion hojaega ek akeli ingress service manifest me

vse lec dkhloge to bht ache sesmjh ajaega, mtlb maza aagya hai mje smjh kr
dkho 3 apps hain backend pe right? to teen alag alg ingress service manifest banengi ab
bas har ingress service manifest ke annotaions me mention krna hoga ke ye kis INGRESS GROUP ka part hai ye manifest
LIKE THIS:
    alb.ingress.kubernetes.io/group.name: myapps.web
    alb.ingress.kubernetes.io/group.order: '10'

suppose 3 app hain agar backend me to fir ingress-group bas merge kr deta hai teeno ingress service manifest ko and kul milakr ek he ALB banta hai
mtlb ki just confusino clear hojae, kam krne me problm na ho, sb kam organized way me hon eslye har backend app ka ek alag 
ingress service manifest banana hai abse(along with their deployment.yml and service.yml)

NOTE: PRIORITY ORDER set krna hota hai har ingress service manifest ka.

https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/08-NEW-ELB-Application-LoadBalancers/08-12-IngressGroups/kube-manifests
sir ke es gitub link pe ingress group ki yml available h

NOTE: teeno he ingress service manifest me AWS LOADBALANCER ka nam same he dalna hai
bcz ingress group bas organiz krne k liye hai

Infact hmare case me to sub-domain vala part bhi same he rakha hai teeno he ingress service manifest me i.e
 ( external-dns.alpha.kubernetes.io/hostname: ingress-groups-demo601.stacksimplify.com )

We can add any number of rules in our ingress service manifest 


Recursively agar manifest ko deloy krna ho to -R tag laga dete hain
kubectl apply -R -f kube-manifests


<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->

ALB-Ingress-Target-type
by-defaul is instance mode he laga hota hai ingress service me(we dont explicitely define it) i.e ALB ke pas request ati hai to vo corresponding nodeport service ki trf route krta hai,then vaha se request POD ki trf route hoti hai
but in case of Target-type: IP
yaha pe ALB ko jo bhi request ati hai vo directly pod ki ip pe chali jati hai

Target-type: IP is required for sticky sessions to work with Application Load Balancer

we need to explicitely define it in ingress service annotaion like this:
  # Target Type: IP
    alb.ingress.kubernetes.io/target-type: ip  

ab socho mere man me bhi ye ques aya ke jab ALB se request directly pod ip pe route ho rhi hain to why do we need to define k8s service(like nodePort or clusterIP?)
but sir ne bataya hai ke nahi, har haal me we have to define one service( may be nodePort or clusterIP)
bcz then only ingress can create target groups in you ALB

es bar hmlog ne NodePort ki jagh clusterIP service ka use kiya tha.so after manifest execution when we hit kubectl get svc
we see the clusterIPs of our pods

In order to get the IP of our pods we will have to write kubectl get pods -o wide
similarly we can verify the same on aws console that jo ALB bana hai usme target groups me jakr, we need to verify ke pod ki ip mentioned hai ya nhi


<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->

Lec122
Internal ALB
ingress service manifest me annotation me ek jagh pe 'alb.ingress.kubernetes.io/scheme: internet-facing' likha tha now we need to change it to 'internal'
but this internal ALB cannot be test from outside internet, so we need to deploy a curl command pod in our k8s so as to test internal Load Balancer Endpoing using curl command


There is a series of commands ,dkh lo lec directly
dhyan rhe ingress manifest me se SSL related annotations(ACM vale,listener 443 vale etc etc) hatane honge bcz internal loadbalancer ki bat ho ri hai yaha


Lec123
after deployment of manifests(3deployment.yml,3 service.yml, and one ingress-service.yml)

ek curl pod create kiya hai, us curl pod ke andar jakr hmare internal ALB ke url ko ping kiya hai content verify kiya hai that sahi se aa rha hai ya nhi
below is the manifest of curl pod creaton
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
spec:
  containers:
  - name: curl
    image: curlimages/curl 
    command: [ "sleep", "600" ]


kubectl get ingress
we will see the endpoint of our internal LB,

then using these commands curl pod ke andar jakr alb ke url ko hit kiya hai
# Will open up a terminal session into the container
kubectl exec -it curl-pod -- sh

# We can now curl external addresses or internal services:
curl http://google.com/
curl <INTERNAL-INGRESS-LB-DNS>

# Default Backend Curl Test
curl internal-ingress-internal-lb-1839544354.us-east-1.elb.amazonaws.com

<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->
lec 125
Fargate ka intro hai, aur spec bataye hain
only pay for compute resources that you use. suppose 4 pods banaye hain fargate pe to there will be 4 fargate profile worker nodess will be created.
fargate me ek pod jo hai vo akela ek fargate instance pe chalta hai.

jaise eks with ec2 me cloud-controller hota tha(in control plan)? vse he eks with fargate ke liye we have Fargate-controllers which runs parallel and this is only
responsible for scheduling pods on fargate profiles.

We can directly  bring our existing pods and deploy them on fargate but the only thing is we need to check regarding Fargate considerations
Sort of limitations smjh lo, Fargate considerations below link pe mil jaegi
https://docs.aws.amazon.com/eks/latest/userguide/fargate.html

some examples of fargate considerations:
Privileged containers aren't supported on Fargate.
Each Pod that runs on Fargate has its own isolation boundary. They don't share the underlying kernel, CPU resources, memory resources, or elastic network interface with another Pod.
Network Load Balancers and Application Load Balancers (ALBs) can be used with Fargate with IP targets only. 
GPUs aren't currently available on Fargate.
You can't mount Amazon EBS volumes to Fargate Pods.

10-12 points ki list hai directly dkhlo jakr 

EKS Deployment options
1.	only ec2 nodegroups (managed ec2 nodes)
2	mixed (managed ec2 nodes and fargate nodes)
3	only fargate (only fargate profiles)

EKS with ec2 me hmlog public and pvt subnet dono me he ec2 instances bana skte the
but fargate me fargate profiles are only created in pvt subnet.

Host:pod ratio is 1:1

Lec 126
[IMPORTANT ARCHITECTURE]
BDSNA.

(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/09-EKS-Workloads-on-Fargate/09-02-Fargate-Profiles-Advanced-YAML)


Makesure to upgrade your eksctl every 15-20 days bcz bht frequently updates atey h usme,
Fargate me nodePOrt service ka koi concept nhi hota hai eslye Ingress service yml file me target-type: ip hona chaiye.
Ab pods par traffic jae hmara uske liye hme koi na koi Loadbalancer to chaiye he hoga(jaise we use to install aws load balancer controller which is responsible
for installing ALB for our cluster )
we will verify our pods of external-dns(ye default namespace me hota hia) and ingress-controller-pod i.e AWS-load-balancer-pod(ye kube-system namespace me hota hai)

Suppose do AZ me kam horha hai, so dono AZ ke pub subnet me to NAT gateway deployed hain.
and dono AZ ke pvt subnet me we will deploye a Fargate profile (with name fp-dev)
So hmare pub subnet me to 

Lec 127
vaise to ye lec bhi dkhoge tbzyada clarity aegi but still commands likh diye hain maine.

Suppose we want to know if there are any existing fargate profiles
eksctl get fargateProfile --cluster <cluster-name>

eksctl create fargateprofile --cluster <cluster_name> \
                             --name <fargate_profile_name> \
                             --namespace <kubernetes_namespace>


# Replace values
eksctl create fargateprofile --cluster eksdemo1 \
                             --name fp-demo \
                             --namespace fp-dev


  


In this yml file namespace is very imp , why bcz fargateProfile create krte time hmne yml me namespace define kr diya hai.
to ab jb bhi koi deployment.yml will be executed with this namespace(mtlb deployment.yml ki metadata me ye namespace mention kr diya agr),
to jo bhi pods banenge vo fargateProfile ke banenge.

FargateProfileExecutionRole is created by-default whenever we create our first FargateProfile in our eks cluster

It is super-required that we provide resources: request: cpu: memory:  and limits: for our fargate profile pods
esko ek trh se mandatory he smjh lo tm .
Since hmlog ek mixed env me kam kr rhe hain eslye nodePort ki bhi service.yml banayi hui hai. agar sirf fargate pe kr rhe hote to service.yml ki zrurt na pdti.

jo ingress-service.yml me annotaion: me hmlog target type define krte hain like this: (alb.ingress.kubernetes.io/target-type: ip)
ye chiz hmlog service level par bhi kar skte hain, mtlb ki jaise jo nodePort service.yml hai usme bhi define kr skte hain instead of ingress ki service file me

Vaise jb bhi mixed env me kam hona ho to fr nodeport service.yml file me he define krna target-type ko, ideally yhi krna chaiye


Lec 128
github pe yaha pe sabhi manifest pardi hai dekh lo(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/09-EKS-Workloads-on-Fargate/09-01-Fargate-Profile-Basic/kube-manifests)
jb hmlg ye sb manifest ko execute kr denge
to fir hmare ec2-nodes bhi deploy hojaenge 
aur 2 fargateprofile bhi deploye hojaenge(bcz deployment me replicas: 2 dali thi)

kubectl get nodes -o wide execute krenge jb
to total 4 nodes dikhenge jisme se 2 ke prefix me 'fargate' likha hoga aur baki do ke me simple node dikhenge

# List Namespaces
kubectl get ns

# List Pods from fpdev namespace
kubectl get pods -n fp-dev -o wide

# List Ingress
kubectl get ingress -n fp-dev

# Access Application
http://fpdev.kubeoncloud.com/app1/index.html


# How to delete fargateprofile(ye command maine lecture se dkh kr uthaya hai lec 128 at 7 min ,github ke readme me nhi diya hua hai)
eksctl delete fargateprofile --cluster <cluster-name> --name <Fargate-Profile-Name> --wait
NOTE: jb hmlog fargateprofile ko delete kr dete hain, to fir asal me ye hota hai ke jo pods the hmare vo hmare normal ec2-worker-nodes pe schedule hojate hain automatically
mtlb ki bas fargateprofile delete hoti hai,underlying pods nhi.

<----------------------------------->

Lec129
[IMPORTANT ARCHITECTURE] of mixed mode



In this lec we are going to Deploy 3 Apps in a mixed Mode
2 Apps to 2 different Fargate Profiles
1 App to EKS EC2 Manged Node Group

Abhi tk hmne jo fargateProfile banayi thi vo using eksctl command line se banayi thi(usme problem ye thi ke agar n number of fargateProfile banani hoti to fir n number times cli command runkrna pdta)
but ab hmlog yml file se eksath do fargateProfile banaenge


bht acha architecture hai ,dkh zrur lo.
2 fargateprofile banaenge hmlog(ek user-management-service and APP2)
2 fargate profile kliye hmlog separate 2 ingress-service.yml(one for UMS one for APP2) banaenge aur ec2-worker-nodes ke liye hmlog separate ingress-service.yml banaenge

to jo UMS vali fargateprofile hai uski apni alag nodePort Service hogi,uski apni alag ExternalName service hogi(basically to connect with AWS RDS)
Similarly APP2 fargateprofile ki apni alag nodeport serviceyml hogi 

ExternalDNS service bhi deploy krni hogi so that hmlog DNS records ko explicitly add kar saken inour route53 and based on given inputs by us.






hmlog ne 3 apps ko (deployed in mixed mode) access krne ke liye 3 alag-alag ingress files banayi
why cant we use a single ingress serviceyml file and route the traffic based on context-path url?
we cant do that because Ingress-cross-namespace is not yet supported in K8s as of now.


Lec130
(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/09-EKS-Workloads-on-Fargate/09-02-Fargate-Profiles-Advanced-YAML)
eksctl.io/usage/    <- es link pe jakr hmlog apiVersion: ka current version check kr skte hain kisi bhi yml file ka

Step1
yml file for deploying fargate profiles:suppose es below file ka naam ye hai: 01-fargate-profiles.yml

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig							<------ alag hai yaha pe kch, we are creating fargateprofile but dkho yaha p ClusterConfig likha hai
metadata:
  name: eksdemo1  # Name of the EKS Cluster
  region: us-east-1
fargateProfiles:
  - name: fp-app2
    selectors:
      # All workloads in the "ns-app2" Kubernetes namespace will be
      # scheduled onto Fargate:      
      - namespace: ns-app2
  - name: fp-ums
    selectors:
      # All workloads in the "ns-ums" Kubernetes namespace matching the following
      # label selectors will be scheduled onto Fargate:      
      - namespace: ns-ums
        labels:
          runon: fargate   

Imp
while defining the fp-ums fargateProfile we have given namespace as well as labels: eska mtlb hai jo bhi resources me dono he chize present hongi bas vhi resource
es fargate profile ke andar aenge i.e name shoud bhi ns-ums as well as label should be runon: fargate (Ex- hmlog ko externalDNS ki yml file banani pdegi
for our ns-ums fargate profile,to usme hmne namespace aur labels dono he mention kiye hain. We will see that yml file in this lec itself)

Step2.
Now we will run this yml file using eksctl command    <---- this is something diff from previous manifest executions.
eksctl create fargateprofile -f kube-manifests/01-Fargate-Advanced-Profiles/01-fargate-profiles.yml


Step3.
jo app1 app2 ums vali deployment aur unki nodeport service ki jo yml files hain unme sabme namespace banakr mention kr diye hain

Sir ne es lec me sari yml files dikhayi hain ek ek karke aur jaha jaha bhi changes induce kiye hain vo sb bhi dikhaya hai.(changes as in inducing namespace etc)

Step4
abtk hmara fargate profile ban chuki hongi.which we created in step 2
we can verify that
# Get list of Fargate Profiles in a cluster
eksctl get fargateprofile --cluster eksdemo1

# Delete Fargate Profile
eksctl delete fargateprofile --cluster <cluster-name> --name <Fargate-Profile-Name> --wait
eksctl delete fargateprofile --cluster eksdemo1 --name fp-app2 --wait
eksctl delete fargateprofile --cluster eksdemo1 --name fp-ums --wait
3-5 min lgta h ek fargate profile ko delet ehone me, make sure to write --wait while deleing fargateprofiles


Lec131
sbhi yml files ko deploy krke test krke dikaya hai(i.e teeno app ke liye unki deployment, service,ingress yml files)
if you want to see all the ingress across all the namespaces then typ
kubectl get ingress --all-namespaces

Sir ne nslookup <sub-domain.domain-name.com> karke verify to krva diya ke sare apps sahi se up and running hain
but sir ne ye bhi smjhaya hai ke kbi kbi yml files ko bar bar delete and re-create krne se dns related issues ajate hain(basically route53 caching issues ajate h kbi kbi)

so to cross check that jo nslookup ke results aa rhe hain vo sahi hain bhi ya nhi ,eslye sir ne directly ALB (total 3 alb bane the)me jakr uske url uthaya
aur nslookup me chala ke dkh liye hai

like this nslookup <alb-url>

<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->


Network LOAD Balancer



<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->

Lec142
[IMPORTANT Architecture]
k8s cluster ecr se images kaise pull karta hai ye sb architecture me dikhaya hai
pehle mje laga tha externalName service ka use kiya hoga but main galat tha, the images are pulled via the NAT gateways deployed in the public subnet of k8s cluster

Lec143
es lec me kiya kuchn nhi hai bas terminologies batayi hain ecr ki(ye sb pehle se heata h mje,ke repos hoti hain ecr me,push pull ke commands hote hain)
docker login krna hota hai,docker install krna pdta hai in order to use CLI

Lec144
me docker image banane k command run kiye hain
and then locally he us image ko run krke dekha hai.
(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/blob/master/10-ECR-Elastic-Container-Registry-and-EKS/README.md)

docker build -t <ECR-REPOSITORY-URI>:<TAG> . 
docker run --name <name-of-container> -p 80:80 --rm -d <ECR-REPOSITORY-URI>:<TAG>

simply docker container run kr diya hai aur localhost pe check kr ke dikha diya hai

ECR repo banate time ek option ata hai vulnerabilities ko scan krne ka ,if we enable it to fr ecr me jb image push kr dete hain hmlog to ECR me vulnerabilities check
hokr ek report ready hojati hai infront of our image
it is a good feature.


Lec145
Sir ne image push kr di thi ecr me , ab sir ne dikhaya hai ke enhone manually k8s ke deployment me jakr image ko change kr diya hai
secondly jo nodeport ki service.yml thi usme annotations: alb.ingress.kubernetes.io/healthcheck-path: /index.html 

kr diya hai

Verify that ec2 worker nodes ke IAM role me ecr se images push pull krne ke liye policies attached hain ya nhi

Lec146
Simply sabhi manifest(deployment,service,ingress ki yml file) ko execute krke , aws console pe ALB TG Listenrs,Route53 DNS records wgreh verify krva diye hain.
sub-domain.domain-name.com pe jakr ye dikha diya hai k sahi se image deploy hogyi hai k8s cluster me.

<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->

Lec147
AWS me CI/CD ko 6 min me bht ache se smjhaya hai,dkhlo directly lec he, bcz notes nhi hai es lec me bas diagram ke sath fatafat smjhaya hai

Lec148
Es lec me kch bhi practical nhi karaya hai bas ye btaya hai ke agle lec me kya kya krenge hmlog
We will write a buildspec.yml which will eventually build a docker image, push the same to ECR Repository and Deploy the updated k8s Deployment manifest to EKS Cluster.
To achive all this we need also create or update few roles
STS Assume Role: EksCodeBuildKubectlRole
Inline Policy: eksdescribe

CodeBuild Role: codebuild-eks-devops-cb-for-pipe-service-role
ECR Full Access Policy: AmazonEC2ContainerRegistryFullAccess
STS Assume Policy: eks-codebuild-sts-assume-role
STS Assume Role: EksCodeBuildKubectlRole

Lec149
esme bas ye check kiya hai AWS-load-balancer-controller and externalDNS deployed hai hmare cluster me ya nhi
Then ek ecr repo banayi hai bas

Lec150
codecommit me repository banakr dikhayi hai
jo bhi code hai ex- docker image,k8s manifest ,etc etc vo sb hmlog code-commit me push kr dete hain
now ab code-commit repo to bana li,but now we need security credentials for this code-commit
uske liye sir apne IAM user me gye(i.e kalyan nam ka tha unka personal IAM user),then security-credentials me jakr ek option ata hai
"security-credentials for code-commit"  -> then click on 'Generate Credentials'

Step3
comeback to code-commit repo and copy the url <git clone ... >to clone this repo to our local sytem

Step4
so pehle locally repo ko clone kr lenge then us local folder me jakr:

ek docker file hai jisme Nginx ki image hai and it copies 'app' folder from local system 
app folder ke andar index.yml hai

we will also create kube-manifest folder in our local repo, this kube-manifest will have deployment,service,ingress ki yml files\\
We will also create buildspec.yml file there

ye sb create krke code-commit ki repo me push kr denge via these commands:
git status
git add .
git commit -am "1 Added all files"
git push
git status



Lec150
Ye lec BDSNA(kai commands hain jo run kiye hain )
code-build he hai jo hmare code ko build krta hai and image ko k8s cluseter me deploy krta hai.
(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/blob/master/11-DevOps-with-AWS-Developer-Tools/README.md)
 
but for that we need to create STS Assume IAM Role for codebuild

4-5 commands run kiye hain in order to create STS Assume Role,crate policy and attach it to role( github ke page se directly dkh lo)


EKS Cluster me ek config-map hota hai ,its name is aws-auth
So the role which we created just above we need to mention it in aws-auth config-map

Basically aws-auth name ki jo config-map file thi cluster me,usme ek role add kr diya hai
enhone cli commands ke through kiya hai eslye dkhne me complicated lag bhale rha hai but vaise ye complicated chiz nhi hai.

Lec151
[IMPORTANT ARCHITECTURE]
code-build ka pura architecture smjhaya hai.

CB compiles our source-code, run unit-tests and produces artifacts.
code-build provides us diff pre-packaged-build-env  for diff prog lang and tools like Maven,gradle infact we can customize CB to induce our owns build-tools etc

Lec152
[IMPORTANT ARCHITECTURE]
practical kch nhi karaya hai lekin code-build ka architecture detail me cover kiya hai
notes banana possibe nhi tha.lEc dkh lo directly

Lec153
ye lec maine khud 2-3 bar dkha hai tb jakr thoda thoda smjh aya hai.
BDSNA
The env variable that are required for our CB are 
REPOSITORY_URI = 180789647333.dkr.ecr.us-east-1.amazonaws.com/eks-devops-nginx
EKS_KUBECTL_ROLE_ARN = arn:aws:iam::180789647333:role/EksCodeBuildKubectlRole
EKS_CLUSTER_NAME = eksdemo1

Es lec me buildspec.yml  ko explain kiya hai, format smjaya hai.
dkh lo imp h 



hmare jo deployment ki manifest this usme jaha pe container ki image define krte the like this image: <image-url>
vha pe manifest me value ki jagh pe CONTAINER_IMAGE likha hua hai. like this image: CONTAINER_IMAGE
and buildspec.yml me bhi ye likha hua hai:
- echo "Update Image tag in kube-manifest..."
- sed -i 's@CONTAINER_IMAGE@'"$REPOSITORY_URI:$TAG"'@' kube-manifests/01-DEVOPS-Nginx-Deployment.yml




Earlier when aws supported docker images were not there then we use to install multiple packages(like kubectl,eksctl,awscli etc etc)
 by giving them in 'command:' section of buildspec.yml
but ab 'aws docker images for codebuild' etna google kroge ,then official doc pe jaoge to github khulega aur usme pre-built dockerfile mil jaegi vha pe

Dockerfile me 'AWS related jo bhi tools(ex- awscli,eksctl etc,aws-iam-authenticator)' downlaoding ke instructions ho usme dkh lena k hmlog ke latest version likha hua hai ya nhi

So kul milake aisa hai ke buildspec.yml me jo command: naam ka section hai uske andar kch bhi nhi likhna hia hme bcz mostly sbkuch already installed hota hai pehle se he
Notes banana band kr diye h es lec bcz possible ni tha buildspec.yml ko bas explain kiya hai es lec me

ya to jo es github link pe buildspec.yml file padi hai usko gpt pe dal ke explaination dkh lo vo zyda simple rhega
and format ka bhi idea lag jaega


Lec155
es lec me aws console pe codePipeline bana kr dikhayi hai
stephan ki trh steps hain  to notes banana possible nhi hai but lec dkh zrur lo tbhi smjh aega
bcz beech me code-build ke liye hmlog ko ek project banane ka option ata hai and usko create krna pdta hai while passing the env variables etc etc .

jo env variable dalne hain vo code-build ka project create krte time dalne hain
bcz I  guess codePipeline me bhi env Variable ka option tha vhi pe but usme nhi daale hain env variable sir ne

Sir ne deploye stage ko skip kar diya hai, bcz buildspec.yml me  he mention kr diya tha post_build: me that kubectl -f apply <kube-manifest.yml>
Secondly eslye bhi codeDEPLOY vale step ko skip kr diya hai bcz usme EKS nam ka koi option bhi nhi show ho rha tha.

Ek chiz aur, sir ne janbujkar ke ye codepipeline fail kari thi, because last me we see some error related to GetAuthorizationToken operation





Lec157
actually me problem ye thi ke jo code-build ka project banate time jo service role banaya tha hmne, usme ECR ko access krne ki policies nhi added thi
Then code-build me jakr, service role me gye, usko select krke ecr ka full access de diya usme

RELEASE-CHANGE ye option select krdene sehmari codePipeline manually run hojati h


Ye pipeline firse ekbar fail hojaegi because hmne code-build ke liye jo STS Role banaya tha usko code-build me attach to kahi kiya he nahi hai
Lec 148 me check kro, 2 IAM role banane ka zikr hua hai i.e one is STS Assume Role and another is IAM ROLE for code-build

WE need to update code-build-role to have access to  STS ASSUME ROLE(which we created using STS ASSUME ROLE POLICY)

Now how to give it? ye alag hai bhaiya ekdm
GOTO IAM ROLE,choose policy, then choose create Policy
then CHOOSE A SERVICE => search for STS
in ACTION => select ASSUMEROLE(Write vale block ke andar milega ye option)
in RESOURCES => select specific role arn and pass the code-build vala iam role ka rolearn

vhi vala rolearn jo abhi env variables me set kiya tha while creating the code-build project
Then last me es policy ka naam dedo koi sa bhi aur save kr do ye policy 


Now go to codebuild service role(ary vhi role jo abhi code-build ka project banate time hmlog ne banaya tha ,usme bas ye policy jakr add krdo)

eske bad jb hm manually apni pipeline ko run krenge to vo chal jaegi
then sir ne verify krke dikha diya hai , that dekho cluster me pods create hogye hain,route53 me entries aagyi hain.
alb create hogya hai, hmara dns entry is working perfectly on browser as our kube-manifests have been deployed succesfully on cluster.

Lec 158

code-build me he ek option hota hai goto Build-details ka
usme jakr artifacts dhundhna ,usme artifacts ki location ka url mil jaega ,,i.e basically s3 url hai vo where our artifacts are stored
sir ne sb verify kra ke dikha diya hai ke dkhoo jo bhi artifacts aye hain those were mentioned in build-logs(jinhe hm realtime me monitor kr rhe the)

Then ek new dns name add krke dikhaya hai in our kube-manifest of ingress-service.yml file
comma laga kr diya hai  new dns
like this: 
#Before
    # External DNS - For creating a Record Set in Route53
    external-dns.alpha.kubernetes.io/hostname: devops.kubeoncloud.com   

#After
    # External DNS - For creating a Record Set in Route53
    external-dns.alpha.kubernetes.io/hostname: devops.kubeoncloud.com, devops2.kubeoncloud.com

then pipeline run krke dikha di hai , ke successfully run hogyi
then k8s manifest wgreh sb delte krdiya hai
pipeline bhi delte kr diya hai
ecr repo delte kri hai and
IAM roles jo banaye the vo bhi delte kriye haal

<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->

Lec 159
bas basic intro diya hai microservice ka

Lec160
[IMPORTANT ARCHITECTURE]
pura workflow dikhaya hai microservice ka ,jo hmlog design krne vale hain
like server side

pvt subnet me hmare 2 worker nodes honge(one in each pvt subnet)
ek workernode k andar do microservice hongi
one UMS(usermanagerment service),other is NS(notification service)
UMS ke liye ingress-service.yml honi chaiye bcz it will face endclient using load balancers etc
sath me externalName service ki madad se RDS me data update deletekrna hoga

NS ke liye ek clusterip-service.yml honi chaiye
aur ek SMTP ki externalNAme service


Lec161
Pre-requisites batayi hain sir ne, that externalDNS(ye default namespace me banta hai, lec107 me kiya tha hmlog ne.) and aws LB controller should be deployed.
Along with the in this particular use case of microservice, we are trying to send email notifications to the users

ek RDS db instance should be up and running.


so SES setup krne ke liye we need SES server ka link(aws console pe miljaega)
we need to generate SMTP credentials(on aws console)
Also we need to verify our email ids, from aur to ke liye we neeed to verify them.
sbkch lec me dikhaya hai, tension mt lo

Lec162
es lec me manifest dikhayi hain basically related to UMS
(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/blob/master/12-Microservices-Deployment-on-EKS/README.md)

sbse pehle RDS se interact krne k liye jo externalName service chiaye uski yml file dikhayi hai
and jo NS vali deployment file hai usme ek env variable hai AWS_SMTP_SERVER_HOST uski value me hmari smtp externalName service ka nam dal diya hai
to es trh se apas me deployment ko aur externalName service ko link kiya hai.

Lec163
NOw hmari UMS vali jo deployment file hai, it needs to call NS-deployment file, using clusterIP service.
so for that it(UMS microservice) will basically call the cluster-ip service and then clusterIP service will call the NS vali microservice or say NS microservice ka pod.
SO practically hua ye hai ke jo UMS vali deployment file thi, vo pehle jasii he thi i.e usme RDS instance ke bare me env variables defined the(jaisa ki pehle ki ums deployment.yml files me hmne dekha hai)
NOTIFICATION-SERVICE-HOST: notification-cluster-ip-service and NOTIFICATION-SERVICE-PORT: 8096 ye dono env variable add kr diye hain

Notification service port ko 8096 he q liya hai sir ne ye check krlo gpt pe

Ye annotaions(present in ingress yml) main janbujkr copy kr rha hun yaha, jisse bar bar nazr pade aur yad hojaenge
  annotations:
    # Load Balancer Name
    alb.ingress.kubernetes.io/load-balancer-name: eks-microservices-demo
    # Ingress Core Settings  
    #kubernetes.io/ingress.class: "alb" (OLD INGRESS CLASS NOTATION - STILL WORKS BUT RECOMMENDED TO USE IngressClass Resource)
    alb.ingress.kubernetes.io/scheme: internet-facing
    # Health Check Settings
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP 
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    #Important Note:  Need to add health check path annotations in service level if we are planning to use multiple targets in a load balancer    
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '15'
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'
    alb.ingress.kubernetes.io/success-codes: '200'
    alb.ingress.kubernetes.io/healthy-threshold-count: '2'
    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'  
    ## SSL Settings
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}, {"HTTP":80}]'
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:180789647333:certificate/d86de939-8ffd-410f-adce-0ce1f5be6e0d
    #alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-1-2017-01 #Optional (Picks default if not used)    
    # SSL Redirect Setting
    alb.ingress.kubernetes.io/ssl-redirect: '443'
    # External DNS - For creating a Record Set in Route53
    external-dns.alpha.kubernetes.io/hostname: services.kubeoncloud.com, ums.kubeoncloud.com




NS microservice ki jo deployment file hai usme ek jagh ye entioned hai
 spec:
      containers:
        - name: notification-service
          image: stacksimplify/kube-notifications-microservice:1.0.0
          ports:
            - containerPort: 8096
          imagePullPolicy: Always

This imagePullPolicy ka mtlb hai, k kbhi bhi image uthao to node me pardi hui existing image mt uthao,always go to docker hub and fetch the most recent image.



Lec164
IN this lec all 7 manifest will be deployed
# List Pods
kubectl get pods

# User Management Microservice Logs
kubectl logs -f $(kubectl get po | egrep -o 'usermgmt-microservice-[A-Za-z0-9-]+')

# Notification Microservice Logs
kubectl logs -f $(kubectl get po | egrep -o 'notification-microservice-[A-Za-z0-9-]+')

# External DNS Logs
kubectl logs -f $(kubectl get po | egrep -o 'external-dns-[A-Za-z0-9-]+')

# List Ingress
kubectl get ingress

Then postman me jakr env create kiya hai so that we can thoroughly test these deployed microservices
Sir ne lecture number bhi mention kiya hai, that postman ko jb pehli bar configure kiya tha to es lec me kiya tha.

sir ne finally demo krke dikhaya h that postman se we can hit the createUserApi in UMS microservice,and ultimately it will ping our clusterIp service there by pinging NS service
due to which we get email notification in our emails.



IMP: jb bhi hm externalName service banate bhi (and suppose pvt subnet me hmare pods hain.) to bhi request 
pehle externalName ko jati hai,then vaha se NAT gateway me, then vaha se ultimately us service pe jisko hm call krna chahte hain

Lec165
3 tarah se we can update the image in our eks deployment
1. set-image option(voi cli command jo pehle padha tha)
2. kubectl-edit
3. update manifest and kubectl-apply

Es link me last me commands diye hue hain
https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/blob/master/12-Microservices-Deployment-on-EKS/README.md

kubectl set image deployment/notification-microservice notification-service=stacksimplify/kube-notifications-microservice:2.0.0 --record=true
es command me jo 'notification-service' likha hai, vo kch nhi h, hmare container ka naam hai.

--record=true kr diya hai esi ki vjh se rollout history me ye vala rollout reflect hoega.
once we do:  kubectl rollout history deployment/notification-microservice then hme dkhne ko milega

IMp: default deployment strategy is rolling update. so we will not have a downtime if we change the image
but we can also customize further this rolling update stragtegy like how much percent or how many pods should get the new load etc. like this


<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->
Lec166
AWS X-Ray
used for analysing and debugging complex applications built using microservice architecture
we can understand how our app and its undrlying services  are performing to identify and troubleshoot the root cause
provides end-to-end view of the request as they travel through our app and shows a map of our application's underlying components.
can be used to analyse simple 3 tier apps as well as complex microservice apps with thousands of services

we can discover request behavior
we can debug issues using xray

it collects data from each underlying application services it passes through
xray combines gathered data from each microservice  into singular untis called traces
we see the traces on service map(that pictorial representation , sbhi services ke aps me kya links hain etc etc )
we also have 'Analyze issues' phase where we can go deep down into a trace and see the flow and what is exactly happening in it.

X-Ray related agent hme deploye krna pdega apne k8s clyster me,we can deploy that using K8s Daemon sets
basically we call it daemonset pod


Lec167 
[IMPORTANT ARCHITECTURE]
DaemonSets
A DaemonSet ensures all or some nodes run a copy of a DaemonSet pod
whenever a new node is added(via clusterAutoscaler) a pod configured as daemonset(aka DaemonSet pod) will be added to that node
and whenever an existing node is scaled-in then those pods are deteletd
Deleting a daemonset will cleanup all the pods configured on all the nodes.

Some typical uses cases of DaemonSet are:
1. running a 'logs collection daemon'  on every node(ex- fluentd)
2. running node monitoring daemon  on every node(like cloudwatch agent)
3. running an application  trace collection daemonset on every node(AWS X-ray)     <- this we will be doing as part of this series of lec

In simple case, ek DaemonSet hoga, for covering all nodes and for each type of daemon. (99% cases me ye use hota hai)

In more complex setup: may be multiple DaemonSet would be deployed for single type of daemon ,but with diff flag/diff memory/diff cpu request /diff h/w types

I am writing down architecutre
So suppose do worker node hain.
to we deploy a daemonset (that will deploy daemonset POD on each worker node)
We also need to deploy XRay-ClusterIP-Service(so that other resources can interact with the daemonset pods)

UMS vali jo pods hain usme we deploye Xray SDK related configuration,
which means these pods are configured in such a way that jo bhi request traffic wgre sb hoega vo sb kch in the form of traces hmare DaemonSet pods(it can goto any daemonset pod deployed on any node)
(i.e UMS pod in workernode2 can send the traffic logs in the Daemon-pod deployed on workernode-1 and vice versa and this is bcz of Standard X-Ray ClusterIP Service) me chala jaega
 and then DaemonSet-pods se info X-Ray service of aws me chali jaegi

Lec168
[IMPORTANT ARCHITECTURE] 
Microservice distributed tracing with AWS X-Ray from a network design perspective 
and also we will see X-Ray service map and traces

whenever we deploy and X-Ray as a DaemonSet,it is going to created X-Ray pods per worker node and then it is going to expose that X-Ray pod using X-Ray ClusterIP

In this architecture, X-Ray ki ek alg se microservice define karni pdegi
UMS and NS me X-Ray related code likh diya hai with SDK to enable the xray tracing for only this https://ums.kubecloud.com/usermgmt/notification-xray microservice
















<------------------------------->
<------------------------------->
<------------------------------->
<------------------------------->

Lec179
[[IMPORTANT ARCHITECTURE]]

We are going to deploy a default kubernetes matrix server, where the pod metrics will be sent to from the pods to the k8s matrix server

When we enable HPA for our application it queries the  "kubernetes matrix server" at every 15 sec ,then it calculates 
the no. of replicas it needs to create then it scales the pod replicas accordingly
Es pure process ko CONTROL LOOP kehte hain.

Scale-out hone me jo new pods banenge unki bhi to metrics hongi vo automatically kubernetes matrix server me jane lagengi

HPA(Horizontal POD Autosclaer)
we need to execute this command: 
kubectl autoscale deployment <deployment-name> --cpu-percent=50 --min=1 --max=10

Lec180
# Verify if Metrics Server already Installed
IMP: dhyan se dkho to kube-system nam ke namespace me sir ne install kiya hai
kubectl -n kube-system get deployment/metrics-server

# Install Metrics Server , agar install krna hai to sir ne ek link diya hai jaha se latest version ka link hmlog de skte hain.

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml
ye kar lene ke bad matrix-server nam ka ek pod ban jata hai hmare kube-system namespace me



# Verify
kubectl get deployment metrics-server -n kube-system

Simple ek deployment aur ek service ki yml file hai es lec me.
but qki hmare worker nodes are present in pvt subnet eslye we cannot access them directly,
i.e hmlog ne kubectl get nodes -o wide kiya to ,but ec2 ki public IP nhi reflect hui,just bcz ec2 was in pvt subnet
(However agar ALB wgreh bhi configure kiya hota to externally bhi acces kr lete)
anyways HPA ko test krne k liye cpu-utilization badhanaa hai hme naa ki externally access krke

# Describe HPA
kubectl describe hpa/hpa-demo-deployment       <- dhyan se dkho thoda alag hai command hpa/<deployment-name> likha hai
esi command se hme apne worker node ki cpu utilization ka pata chalta hai

 
# List HPA
kubectl get horizontalpodautoscaler.autoscaling/hpa-demo-deployment           <- dhyan se dkho thoda alag hai command ,
esi command se hme apne worker node ki cpu utilization ka pata chalta hai


Lec181
Es lec me apache-bench command ki madad se worked node par load dalna dikhaya hai.
# Generate Load
kubectl run --generator=run-pod/v1 apache-bench -i --tty --rm --image=httpd -- ab -n 500000 -c 1000 http://hpa-demo-service-nginx.default.svc.cluster.local/
ya to lec dkh kr ye command smjhne ki koshish kro ya fir gpt se dkh lo

Default cooldown period is 5 minutes.
Once CPU utilization of pods is less than 50%, it will starting terminating pods and will reach to minimum 1 pod as configured.

k8s  ke further versions(1.18) se HPA ke declarative methods ajaenge mtlb ki HPA ke liye proper yml file likh kr ache se declare krna hoga

GPT se maine HPA ki ek yml file le hai
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: your-hpa
  namespace: your-namespace
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: your-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 180
      policies:
      - type: Pods
        value: 2
        periodSeconds: 30
    scaleDown:
      stabilizationWindowSeconds: 180
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
		
		
<------------------------------------------------------->
<------------------------------------------------------->
<------------------------------------------------------->
<------------------------------------------------------->
<------------------------------------------------------->
Lec182
VPA(Vertical POD autoscaler)
unlike HPA(which is a default k8s resource),VPA needs to be installed explicitely in our cluster

lec183
VPA ke demo dene k liye sir ne jo normal deployement hota hai usme
resources:
  requests:
	cpu: "5m"
	memory: "5Mi"
	
bas 5mebibytes memory li hai,jisse ache se vpa ka demo de saken.



apiVersion: "autoscaling.k8s.io/v1beta2"
kind: VerticalPodAutoscaler
metadata:
  name: kubengix-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: vpa-demo-deployment
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        minAllowed:
          cpu: 5m
          memory: 5Mi
        maxAllowed:
          cpu: 1
          memory: 500Mi
        controlledResources: ["cpu", "memory"]
		
		
lec184
yml file hai VPA ki usko smjhaaya hai and then execute kiya hai uske bad
kubectl get vpa

<------------------------------------------------------->
<------------------------------------------------------->
<------------------------------------------------------->
<------------------------------------------------------->
<------------------------------------------------------->

Lec185
ClusterAutoccaler
It basically scales out when required and  scalesin when there are underutilized worker nodes
Just intro diya hai cluster Autoscaler ka 1 min me es lec me aur kch nhi hai

VPA ki trh esko bhi explicitely install krna pdta hai.

Lec186
BDSNA
we have to make sure that worker nodegroup banate time --asg-access hmlog ne diya tha ya nhi diya tha
Nodegroup ka jo IAM role hai usme jo IAM policy hai usko verify krke dikhaya hai that ASG access hai hmare nodegrup ke IAM role ke pas

(1)# Deploy the Cluster Autoscaler to your cluster
kubectl apply -f https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml

Ye above command run kr dene se hmare k8s me (kube-system namespace me) clusterAutoscaler ka ek deployment ban jata hai

(2)now we have add the cluster-autoscaler.kubernetes.io/safe-to-evict annotation to the deployment
kubectl -n kube-system annotate deployment.apps/cluster-autoscaler cluster-autoscaler.kubernetes.io/safe-to-evict="false"

(3)eske bad clusterAutoscaler ki ek deployment file ban kr ajati hai , we need to perform certain changes inthat file like mentioning our clustername
and adding these two parameters as stated belo
 - --balance-similar-node-groups
 - --skip-nodes-with-system-pods=false
 
(4)Then we need to Set the Cluster Autoscaler Image related to our current EKS Cluster version


Ye link hai github page ka(https://github.com/harshitpandey8587/aws-eks-kubernetes-masterclass/tree/master/17-EKS-Autoscaling-Cluster-Autoscaler)
yaha se directly padh lo commands qki usme zyada elaborative way me smjh aega

ye lec vaise bhi BDSNA

Make sure to Set the Cluster Autoscaler Image related to our current EKS Cluster version



Lec187
clusterAutoscaler ko scale-in karne k liye hmekuchnhi krna pdta hai,jaise jaise replicas kam hoti jatihain vo automatically scale-in kr leta hai.
Es lec me ek deployment execute kiya hai, deployment me jo container hai uske resources:
requests:
	cpu: 200m
	memory: 200Mi
	
	rakhi hai ,jisse jo bhi pod bnega vo by default es config ke sath bnega atleast.
	
Then manually kubectl scale command ki help se replicas increase kr di hain: kubectl scale --replicas=30 deploy ca-demo-deployment 

esse ye hoga ke ek worker node pe jab already kai pods ban jaenge to clusterAutoscaler automatically dusra worker node create kr dega
aur hmare upcomig pods uspe ban jaenge

but remember ke eksctl create node group command me ek jagh pr hm --nodes-max=<value> dete hain
to agar hmare pods ke scaleout hone ki vjh se already worker nodes max number ban chuke hain to remaining pods jo bhi banne hain vo sb scheduling mode me chale jaenge


ek chiz aur dhyan rakhna ke ye jo clusterAutoscaler ki deployment file hmlog ne install kari this
usko jb bhi access krenge to deployment.app/<cluster-autoscaler> karke karenge
i mean ye koi simple si deployement file nhi hai jo hmlog kubectl get deployment <deployement-name> likh kr fetch kar len
