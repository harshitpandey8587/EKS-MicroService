Lec-8
EKS cluster ke 4 components hote hain..
1.Control plane
2.Worker nodes(EC2 instances that are managed by us)
3.Fargate Profiles (Instead of Ec2 we can run our app workload on fargate profiles)
4. VPC (although ye part nai hota hai EKS ka but still it plays a very crucial role eslye we counted vpc here)
$$$$$$$$$$$$$$$$$$$$$$
->Fargate profiles only run on private subnets i.e minimum 1 pvt subnet hona he chaiye in our vpc if we want to run fargate profiles.
->Suppose hmare worker node jo hain vo pvt subnet me hain so in order to establis a connection between such worker nodes and control plane NAT Gateways setup
krna pdta hai. 
$$$$$$$$$$$$$$$$$$$$$$
Broader level pe jo EKS ka architecture hota hai:
EKS Control plane has atleast 2 api servers and 3 etcd which runs across three AZ within a region.
EKS control plane automatically detects unhealthy "Control plane" instances and replaces them.

Worker Nodes i.e EC2 instances
A node group is one or more ec2 instances deployed in an Ec2 ASG.
All instances in a node group should have same instance type,same AMI and use the same "EKS worker node IAM Role"

Fargate:
Fargate is serverless  ye sb to pata hai.
AWS Specially built Fargate controllers whose work is to identify  the pods belonging to fargate  and schedules them on fargate profiles

VPC
VPC is used for traffic flow from worker nodes to Control plane within the cluster,
EKS Control planes are highly isolated from other control planes present in the same AWS account.

<----------------------------------------->
<----------------------------------------->
<----------------------------------------->

Lec-9

Simple EKS cluster banane ka command run kiya hai, it takes around 15-20 min to create this.
Fir hme "IAM OIDC provider for our EKS Cluster" ko banakr associate krna hota hai, AWS Console me krne jaenge to bht sare steps hote hain eske (it is possible
that we make mistake there, eslye we will simply run a command on cli)


<----------------------------------------->
<----------------------------------------->
<----------------------------------------->

->jb bhi hm eks create cluster  ya fir eksctl create nodegroup ye sb command run krte hain to api call jata hai CloudFormation ko
Cloudformation he backend pe hmara ye sb cluster/worker nodes create krta hai.

-> Worker nodes me agar hme ye dekhna hai ke kon kon se inbound ports allowed hain to we need to see that security group jiske nam me "remote"
likha hai.

-> worker nodes ko outside internet se koi bhi insan access kr paye so for that we need to allow traffic All traffic from Anywhere 0.0.0.0/0
-> Nodeport service basically dynamic ip generate krta hai for our worker nodes and we are able to access them using
Worker_node_public_ip:dynamic_port

<---------------------------------->

Lec12 in Sec2 me pricing smjhayi hai EKS ki. Its imp becz sbi log interview me cost ki trf dhyan dete hain.

->Just to Note: Worker nodes ko hm normal ec2 instance ki trh stop/start nahi kr skte, so we need to delete the worker nodes(node group) if we are
not using it.

->Cluster deletion ke liye ek bht bht bht imp point hai ke kbi kbi hme agar apne cluster delete krna hai , and suppose kuch changes hmne 
manually bhi kar rakhe hain(to our resources that are created by EKS) apne aws console me EKS cluster me jakr, 
to first we need to roll back our changes(Like SG me ports allow kiye the, to
first we need to delete those rules manually, uske bad he hme cluster delete vala command chalana hai: eksctl delete cluster <cluster-name>)
It is v imp to roleback our manual changes before deleting the cluster


agar hm ye krna bhul jate hain to fir hme cloudformation me jakr manually bache hue resources ko ek ek krke delete krna hoga.
<---------------------------------->
<---------------------------------->
<---------------------------------->

Lec 19
->Suppose Docker desktop download krliya windows/macOS me. fir ye command run kiya:
docker run --name app1 -p 80:8080 -d stacksimplify/dockerintro-springboot-helloworld-rest-api:1.0.0-RELEASE


->here 80 means hmare desktop ka port 80 ko hmne container ke port number 8080 se map kr diya hai.
i.e First port is LOCAL PORT and the second 8080 is the CONTAINER PORT

->V Imp to Note: So jo hmare system me localhost hota hai uska port hota hai 80
So jb hm map kr denge apne 80 ko container ke 8080 se, fir apne system pe hmne agar ye run kiya http://localhost/hello
to fir hm apne container ko access kr paenge.

-> if we write docker ps -a -q  , to hme container id mil jaengi directly of the stopped container without showing any sort of faltu ki info about the container

Lec20 me bhi Localhost ka exampole diya hai. 
Image ki retagging bhi dikhai hai, ke suppose hme koi image banayi docker_hub_account/image_name:v1
bad me hme laga ke v1 acha nahi lag rha to we retagged it to v1-Release and then we pushed this newly retagged image to docker hub

Lec21
docker stats command se we can display live stream of container resource usage strategy
docker top container-name se we can display the running process of a container
<---------------------------------->
<---------------------------------->
<---------------------------------->
Lec22
->at 3 min, Controller Manager ke components bataye hain ye mujhe nahi pta the pehle se: 
->Control plane ka ek component aur bhi aur bhi hota hai apart from Controller Manager, etcd, kube-scheduler,api-server: Cloud Controller manager
->It is basically for cloud, on-premise k8s infra me ye component nahi hota hai.
->Eske i.e cloud controller manager ke 2-3 components hote hain, vo directly video me jakr dekh lo.

->99% cases me ek pod me ek he container hota hai, (it is always recommended to have so)
kbi kbi ek pod me ek se zyada container hote hain to we call such containers : side-cars
->These side cars are used to support main container ex-main contianer k liye data pull krke lane ke liye, main container ke logs kahi push krne ke liye etc etc

Lec27
->single pod creation with just a command dikhaya hai i.e without creating any manifest. at 2:50
		->kubectl run <container-name> --image <repo-name/image-name>:<tag>

->EKS me troubleshooting ke liye "describe" command is very imp, because usse we can get to know the events occured in the pod etc etc aur sath me
 bht si details mil jati hain pod ke bare me
 
Lec28
->3 ways se we can expose our pods to outside: ClusterIP(pods will be accessible within cluster),NodePort,LoadBalancer(Specifically used with Cloud platforms)
-> NodePort service jo hai usme IP kis hisab se kam krti hain ye btaya hai at  2 min ke as pas 

Lec29 me Handson dikhaya hai NodePort service ka
CLI se ek command me he agar kisi pod ko NODEPORT service ke through internet ko expose krna hai to uske liye ye command use kar skte hain:
		->kubectl expose pod <Pod-Name>  --type=NodePort --port=80 --name=<Service-Name>
		->kubectl expose pod my-first-pod  --type=NodePort --port=80 --name=my-first-service

Fir "kubectl get service" karke check bhi kar skte hain ke hmari service bani ya nahi bani
Now we can access our pod from internet using <public-ip-of-workernode>:<Node-port>					(nodeport mtlb vo jo 30000-32767 ke beech ki value thi)
Service jo hoti hai vo accross worker nodes pe jati hai i.e jitne bhi worker nodes honge sbpe ek sath implement hoti hai(na ki kisi ek individual worker node pe)
.
