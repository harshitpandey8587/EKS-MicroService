Lec-8
EKS cluster ke 4 components hote hain..
1.Control plane
2.Worker nodes(EC2 instances that are managed by us)
3.Fargate Profiles (Instead of Ec2 we can run our app workload on fargate profiles)
4. VPC (although ye part nai hota hai EKS ka but still it plays a very crucial role eslye we counted vpc here)
$$$$$$$$$$$$$$$$$$$$$$
->Fargate profiles only run on private subnets i.e minimum 1 pvt subnet hona he chaiye in our vpc if we want to run fargate profiles.
->Suppose hmare worker node jo hain vo pvt subnet me hain so in order to establis a connection between such worker nodes and control plane NAT Gateways setup
krna pdta hai. 
$$$$$$$$$$$$$$$$$$$$$$
Broader level pe jo EKS ka architecture hota hai:
EKS Control plane has atleast 2 api servers and 3 etcd which runs across three AZ within a region.
EKS control plane automatically detects unhealthy "Control plane" instances and replaces them.

Worker Nodes i.e EC2 instances
A node group is one or more ec2 instances deployed in an Ec2 ASG.
All instances in a node group should have same instance type,same AMI and use the same "EKS worker node IAM Role"

Fargate:
Fargate is serverless  ye sb to pata hai.
AWS Specially built Fargate controllers whose work is to identify  the pods belonging to fargate  and schedules them on fargate profiles

VPC
VPC is used for traffic flow from worker nodes to Control plane within the cluster,
EKS Control planes are highly isolated from other control planes present in the same AWS account.

<----------------------------------------->
<----------------------------------------->
<----------------------------------------->

Lec-9

Simple EKS cluster banane ka command run kiya hai, it takes around 15-20 min to create this.
Fir hme "IAM OIDC provider for our EKS Cluster" ko banakr associate krna hota hai, AWS Console me krne jaenge to bht sare steps hote hain eske (it is possible
that we make mistake there, eslye we will simply run a command on cli)


<----------------------------------------->
<----------------------------------------->
<----------------------------------------->

->jb bhi hm eks create cluster  ya fir eksctl create nodegroup ye sb command run krte hain to api call jata hai CloudFormation ko
Cloudformation he backend pe hmara ye sb cluster/worker nodes create krta hai.

-> Worker nodes me agar hme ye dekhna hai ke kon kon se inbound ports allowed hain to we need to see that security group jiske nam me "remote"
likha hai.

-> worker nodes ko outside internet se koi bhi insan access kr paye so for that we need to allow traffic All traffic from Anywhere 0.0.0.0/0
-> Nodeport service basically dynamic ip generate krta hai for our worker nodes and we are able to access them using
Worker_node_public_ip:dynamic_port

<---------------------------------->

Lec12 in Sec2 me pricing smjhayi hai EKS ki. Its imp becz sbi log interview me cost ki trf dhyan dete hain.

->Just to Note: Worker nodes ko hm normal ec2 instance ki trh stop/start nahi kr skte, so we need to delete the worker nodes(node group) if we are
not using it.

->Cluster deletion ke liye ek bht bht bht imp point hai ke kbi kbi hme agar apne cluster delete krna hai , and suppose kuch changes hmne 
manually bhi kar rakhe hain(to our resources that are created by EKS) apne aws console me EKS cluster me jakr, 
to first we need to roll back our changes(Like SG me ports allow kiye the, to
first we need to delete those rules manually, uske bad he hme cluster delete vala command chalana hai: eksctl delete cluster <cluster-name>)
It is v imp to roleback our manual changes before deleting the cluster


agar hm ye krna bhul jate hain to fir hme cloudformation me jakr manually bache hue resources ko ek ek krke delete krna hoga.
<---------------------------------->
<---------------------------------->
<---------------------------------->

Lec 19
->Suppose Docker desktop download krliya windows/macOS me. fir ye command run kiya:
docker run --name app1 -p 80:8080 -d stacksimplify/dockerintro-springboot-helloworld-rest-api:1.0.0-RELEASE


->here 80 means hmare desktop ka port 80 ko hmne container ke port number 8080 se map kr diya hai.
i.e First port is LOCAL PORT and the second 8080 is the CONTAINER PORT

->V Imp to Note: So jo hmare system me localhost hota hai uska port hota hai 80
So jb hm map kr denge apne 80 ko container ke 8080 se, fir apne system pe hmne agar ye run kiya http://localhost/hello
to fir hm apne container ko access kr paenge.

-> if we write docker ps -a -q  , to hme container id mil jaengi directly of the stopped container without showing any sort of faltu ki info about the container

Lec20 me bhi Localhost ka exampole diya hai. 
Image ki retagging bhi dikhai hai, ke suppose hme koi image banayi docker_hub_account/image_name:v1
bad me hme laga ke v1 acha nahi lag rha to we retagged it to v1-Release and then we pushed this newly retagged image to docker hub

Lec21
docker stats command se we can display live stream of container resource usage strategy
docker top container-name se we can display the running process of a container
<---------------------------------->
<---------------------------------->
<---------------------------------->
Lec22
->at 3 min, Controller Manager ke components bataye hain ye mujhe nahi pta the pehle se: 
->Control plane ka ek component aur bhi aur bhi hota hai apart from Controller Manager, etcd, kube-scheduler,api-server: Cloud Controller manager
->It is basically for cloud, on-premise k8s infra me ye component nahi hota hai.
->Eske i.e cloud controller manager ke 2-3 components hote hain, vo directly video me jakr dekh lo.

->99% cases me ek pod me ek he container hota hai, (it is always recommended to have so)
kbi kbi ek pod me ek se zyada container hote hain to we call such containers : side-cars
->These side cars are used to support main container ex-main contianer k liye data pull krke lane ke liye, main container ke logs kahi push krne ke liye etc etc

Lec27
->single pod creation with just a command dikhaya hai i.e without creating any manifest. at 2:50
		->kubectl run <container-name> --image <repo-name/image-name>:<tag>

->EKS me troubleshooting ke liye "describe" command is very imp, because usse we can get to know the events occured in the pod etc etc aur sath me
 bht si details mil jati hain pod ke bare me
 
Lec28
->3 ways se we can expose our pods to outside: ClusterIP(pods will be accessible within cluster),NodePort,LoadBalancer(Specifically used with Cloud platforms)
-> NodePort service jo hai usme IP kis hisab se kam krti hain ye btaya hai at  2 min ke as pas 

Lec29 me Handson dikhaya hai NodePort service ka
CLI se ek command me he agar kisi pod ko NODEPORT service ke through internet ko expose krna hai to uske liye ye command use kar skte hain:
		->kubectl expose pod <Pod-Name>  --type=NodePort --port=80 --name=<Service-Name>
		->kubectl expose pod my-first-pod  --type=NodePort --port=80 --name=my-first-service

above command me we only gave service port i.e 80,basically system assumes here ke target port(i.e pod ka port) bhi 80 hai.
Fir "kubectl get service" karke check bhi kar skte hain ke hmari service bani ya nahi bani
Now we can access our pod from internet using <public-ip-of-workernode>:<Node-port>					(nodeport mtlb vo jo 30000-32767 ke beech ki value thi)
Service jo hoti hai vo sare worker nodes pe jati hai i.e jitne bhi worker nodes honge sbpe ek sath implement hoti hai(na ki kisi ek individual worker node pe)

->agar hme service port kuch aur rakhna hai aur target port(i.e container port) kuch aur to fir hm vo bhi kar skte hain but uske liye hme alag se target port mention
karna pdega apne command me
		->kubectl expose pod my-first-pod  --type=NodePort --port=81 --target-port=80 --name=my-first-service3

-> We can cross check services by kubectl get svc ya fir kubectl get service


Lec30
Realtime me pod ke logs ko kaise dekhte hain ye dikhaya hai. Infact 1:45 min pe ek link dikhaya hai where we can see different options
used with kubectl log command, which is very useful while troubleshooting any pod

Connect to any pod
		->kubectl exec -it <pod-name> -- /bin/bash
Bina container ke andar jaye bhi hm container me commands run kr skte hain: kubectl exec -it <pod-name> <jo-bhi-command-chalana-ho>

		->Kisi bhi pod ki yaml file agar dekhni ho: kubectl get pod <pod-name> -o yaml
Similarly kisi bhi service(ClusterIP,nodeport etc) ki agar yml file dekhni ho to we can execute:  kubectl get service <service-name> -o yaml

Lec31
Deletion dikhaya hai pod ka aur services ka.
->if you want to get all the objects present in a namespace: kubectl get all


Lec32
Suppose hmare k8s pe hmne koi app host kr rakha hai, aur achank se traffic badh jata hai to we can handle the traffic by:
1-Update ReplicaSet number,2-We can use kubectl scale command to do that, 3-We can enable the HorizontalPodAutoscaler to autoscale the pods 

Lec32
		-> to get the replicaset: kubectl get replicaset 

Further ye dikhaya hai ke RepicaSet ko as a Service kaise expose krte hain.
Basically we create a service here.


Further lectures me  Replica set ko badhana dikhaya hai (i.e Number of desired pods), and  internet ke through access krna dikhaya hai. 
"ReplicaSet" and "Service" ko Delete krna dikhaya hai, Github pe doc ko check krlo usme ekdm saf saf achese commands likhe hain. 


Lec35
Whenever we create a Deployment bedefault a ReplicaSet is rolled out.
Deployment maintains the version of our app, it stores last 10 versions by default
Rollout and Rollback options are provided.
We can scale scale up scale down
We can pause and resume a deployment (i.e agar hmeapne app me suppose 10 changes krne hain to we dont want ke ek change kre fir sare pods ko recreate karen
fir dusra change kare fir recreate, eslye pausing a deployment means, deployment ko pause krke eksath changes krlo ya fir ek ek krke he krlo jaisa theek lage
then deployment ko resume krdo,usse vo sare changes hmare container me reflect hojaenge)
Canary Deployment i.e live production environment me kisi change ko introduce krna

Lec36
Deployement create krne ke bad ek he command se hm apne ReplicaSet ko scale up and scale down kar skte hain:
		-> kubectl scale --replicas=10 deployment/my-first-deployment 
		->kubectl get deploy   (es command se we can see our deployment)
Then Deployment ko as a Service expose krna dikhhaya hai (ek he command sehojata hai. {Similarly abi upar jaise replicaset ko expose kiya tha})

Sir ne Deplyoment padhate time total 4 docker images ka use kiya hai so that vo hme version change krna i.e Rollback etc dikha saken.

Lec37
Application ki image ko update krna sikhya hai from version1 to version2 using set image, All these things are imp. 
chaho to directly github repo se he padh lo
		->#Get Container Name from current deployment
		->kubectl get deployment my-first-deployment -o yaml

		->#Update Deployment - (Upar vale command se hmne container name nikal lenge,fir usko neeche vale command me use kr lenge,dhyan rahe we talking about Container Name and not Pod Name)
		->kubectl set image deployment/<Deployment-Name> <Container-Name>=<Container-Image> --record=true
		
jab bhi hm docker image ka version change krte hain ,basically ek new update rollout krte hain to fir by default ek new ReplicaSet create hojata hai.
we can see it by "kubectl get rs"


Lec38
Updating application using Edit Deployment Approach
		->kubectl edit deployment/<Deployment-Name> --record=true
Kbi bhi deployment version history ko store krna ho to we write  --record=true, esse versioning ki details store hojati hain.

Similarly jb hm teesra update dalenge apne deployment me hmare pas ek ReplicaSet automatically ban jaega firse
Note: jb hm kubectl get rs karte hain to hme ReplicaSets ke nam show hojate hain, and jb hm "kubectl get pods" karte hain to hme pods ke nam
dhyan dene vali bat ye hai ke pods ke nam me replicaSets ke naam ka bhi ek hissa hota hai.	

Lec 39
Maine ab ye approach start kr di hai -> main pehle github ka page khud padh leta hun fir uske bad 2x ki speed pe lecture dekh leta hun
Yahan pe ek ek command likhne ka fayda nahi hai qki github ke page pe bht bht bht 2
che se ,saf saf aur kafi informative way me likha hai.
to I suggest ke usi page ko directly refer krlo.
es lecture me Rollback krna dikhaya hai.

Lec 40
Pausing deployement and making all changes and then resuming the deployment.
Note: jab bhi hm kubectl rollout history deployment/<our-deployment-name> 
likhte hain to jo sbse last me record ata hai vahi apna current deployment hota hai.

Ek bar pause command chala dene ke bad: kubectl rollout history deployment/<Deployment-Name>
agar hm koi change krte bhi hain(ex-deployment version badal dena i.e deployment me koi new image dal dena/ ya fir ek change ka example hai-cpu and memory
pe cap laga dena for our deployment) to hmare containers restart nahi hote hain


Lec41
Ye imp hai ,dekh lo direct lec, coz esme bataya hai ke jaise k8s me koi app hosted hai,use agar aws RDS se bat krni hogi to we need to setup "externalName" service
aur agar pods ko apas me within cluster bat krni hogi to we need to setup ClusterIP service
similarly outside users ke liye NodePort service/Ingress/LoadBalancer Service 

Lec42
Industry job ke liye ye lec imp hai bht, esme k8s ke namespaces ke bare me bhi mention kiya hai,
Basically nginx ki conf file me backend deployment ka pura naam:8080 dena hota hai.
to since we are in the same namespace eslye sirf deployment ka nam dene se kam chal jata hai but if we are in diff namespace to fir pura Cluster DNS hostname dena pdta hai.
Yahi ek lecture imp hai. in practical terms. thosa sa confused hun main that how backend and frontend containers got connected?

<---------------------------------->
<---------------------------------->
<---------------------------------->

Lec44
do tarah se list ko define krna dikhaya hai Manifest me, ye mujhe pehle se nai ata tha, mtlb confusition that ab clear hogya ahi

Lec 45
apiVersion v1 likhte hain hmlog manifest me, to vo kon sa version chalra hai currently ye kaha se dkhte hain ye dikhaya hai esme.

Lec46/47/48 me Yaml file banana dikhaya hai, Simple pod ka, ReplicaSet ka, Services(NodePort) ka,Deployment ka
mje ye nai pata tha ke Deployemnt.yml alag banti hai and Services.yml alag , I mean hme agar kisi RS/Deployement ko agar as a Service expose krna hai
to uske liye specifically hme alag se ek services nam ki yml file banani hoti hai.
ye lectures kafi similar hain Bhupendra Rajput se, but still informative hain.

Ek sath 2 yml files bhi hm run kr skte hain: kubecctl -f apply file.yml -f file2.yml


Lec50
Es lecture me Deployment and Services ki Yaml file bana ke dikhai hai, This lec is imp for writing manifest in K8s
jaise har chez ka ek  profarma hota hai ke services ki yml banaenge to usme kya kya chiz hoti hai ..
Vaise he Deployment ka pattern kch thooooda sa alag hota hai.(Ex- Deployment yml me replicas,selector,template ye teen object hote hain)(Vaise he Services ki yml file me type,selector,ports yeteen object hote hain spec: ke neeche)
2x ki speed pe dkh lo lect, ache se pattern smjh ajaega
Mtlb jaise kisi POD ki yml file me alag trah se port ko allow kiya jata hai, vahi kis Services ki Yaml me Ports ko kisi alag trah se define kiya jata hai yaml file me.
(Yad hai Services ko export krne me --port aur --target-port define krna pdta tha? nahi yad hai to refer line number 129 in this file)


When we are writing Backend Cluster service port(i.e apne backend ek pods ke liye jo bhi service(eg ClusterIP type ki) vali yml likhenge usme name of the  Service is very imp, 
qki hmare case me hmne front me es hisab se coding kari hai ke it will only route the traffic to a service jiska naam "my-backend-service" hoga)
